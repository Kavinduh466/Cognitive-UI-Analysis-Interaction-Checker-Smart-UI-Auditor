{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e686b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new Ultralytics Settings v0.0.6 file ✅ \n",
      "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
      "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
      "Torch version: 2.5.1+cu121\n",
      "YOLO version: 8.3.241\n",
      "GPU Available: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import ultralytics\n",
    "print(f\"Torch version: {torch.__version__}\")\n",
    "print(f\"YOLO version: {ultralytics.__version__}\")\n",
    "print(f\"GPU Available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "727ab0f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 'merged_industry_map.csv' created successfully!\n",
      "Sample mapping:    UI Number                 Category\n",
      "0          0                  Medical\n",
      "1          1                  Medical\n",
      "2          2  Video Players & Editors\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the raw files we downloaded earlier via wget\n",
    "ui_details = pd.read_csv(\"ui_details.csv\")\n",
    "app_details = pd.read_csv(\"app_details.csv\")\n",
    "\n",
    "# Merge on 'App Package Name' to connect UI Numbers to Categories\n",
    "merged = pd.merge(ui_details, app_details, on='App Package Name')\n",
    "\n",
    "# Save the exact file your Harvester is looking for\n",
    "merged.to_csv(\"merged_industry_map.csv\", index=False)\n",
    "\n",
    "print(\"✅ 'merged_industry_map.csv' created successfully!\")\n",
    "print(f\"Sample mapping: {merged[['UI Number', 'Category']].head(3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54db48a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---  AI Harvester Active on cuda:0 ---\n",
      "--- Phase 1: Processing Rico (Mobile) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 66261/66261 [24:25<00:00, 45.20it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Phase 2: Processing YashJain (Web) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Web train: 100%|██████████| 544/544 [00:39<00:00, 13.71it/s]\n",
      "Web val: 100%|██████████| 71/71 [00:05<00:00, 13.72it/s]\n",
      "Web test: 100%|██████████| 76/76 [00:05<00:00, 13.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---  HARVEST COMPLETE! ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "# --- CORRECTED PATHS ---\n",
    "MODEL_PATH = \"best.pt\"\n",
    "RICO_IMAGE_DIR = \"combined_images/combined\"  # Pointing to the nested 'combined' folder\n",
    "WEB_IMAGE_DIR = \"web_data_raw\"\n",
    "METADATA_PATH = \"merged_industry_map.csv\"\n",
    "OUTPUT_BASE = \"Expert_Library\"\n",
    "CONF_THRESHOLD = 0.5\n",
    "MAX_RICO_PER_CAT = 1500 \n",
    "\n",
    "# --- INITIALIZE YOLO ---\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model = YOLO(MODEL_PATH).to(device)\n",
    "print(f\"---  AI Harvester Active on {device} ---\")\n",
    "\n",
    "# PROCESS MOBILE UI IMAGES\n",
    "def process_mobile():\n",
    "    print(\"--- Phase 1: Processing Rico (Mobile) ---\")\n",
    "    df = pd.read_csv(METADATA_PATH, low_memory=False)\n",
    "    cat_counts = {}\n",
    "\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        category = str(row['Category']).replace(\" \", \"_\").replace(\"&\", \"\")\n",
    "        if cat_counts.get(category, 0) >= MAX_RICO_PER_CAT: continue\n",
    "        \n",
    "        ui_id = row['UI Number']\n",
    "        # Rico images in unique_uis are usually .jpg\n",
    "        img_path = os.path.join(RICO_IMAGE_DIR, f\"{ui_id}.jpg\")\n",
    "        if not os.path.exists(img_path):\n",
    "            img_path = img_path.replace(\".jpg\", \".png\")\n",
    "            if not os.path.exists(img_path): continue\n",
    "\n",
    "        results = model.predict(source=img_path, imgsz=1024, conf=CONF_THRESHOLD, verbose=False)\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is None: continue\n",
    "        \n",
    "        # Cropping each detected element\n",
    "        saved = False\n",
    "        for r in results:\n",
    "            for i, box in enumerate(r.boxes):\n",
    "                label = model.names[int(box.cls[0])]\n",
    "                target_dir = os.path.join(OUTPUT_BASE, category, \"Mobile\", label)\n",
    "                os.makedirs(target_dir, exist_ok=True)\n",
    "                \n",
    "                x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "                crop = img[y1:y2, x1:x2]\n",
    "                if crop is not None and crop.size > 0:\n",
    "                    cv2.imwrite(os.path.join(target_dir, f\"m_{ui_id}_{i}.jpg\"), crop)\n",
    "                    saved = True\n",
    "        if saved: cat_counts[category] = cat_counts.get(category, 0) + 1\n",
    "# PROCESS WEB UI IMAGES\n",
    "def process_web():\n",
    "    print(\"--- Phase 2: Processing YashJain (Web) ---\")\n",
    "    for subset in ['train', 'val', 'test']:\n",
    "        # Pointing to the 'images' subfolder inside each split\n",
    "        subset_img_path = os.path.join(WEB_IMAGE_DIR, subset, \"images\")\n",
    "        if not os.path.exists(subset_img_path): continue\n",
    "        \n",
    "        images = [f for f in os.listdir(subset_img_path) if f.lower().endswith(('.png', '.jpg'))]\n",
    "        for img_name in tqdm(images, desc=f\"Web {subset}\"):\n",
    "            img_path = os.path.join(subset_img_path, img_name)\n",
    "            \n",
    "            results = model.predict(source=img_path, imgsz=1280, conf=CONF_THRESHOLD, verbose=False)\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is None: continue\n",
    "            \n",
    "            # Crop and save web UI components\n",
    "            \n",
    "            for i, box in enumerate(results[0].boxes):\n",
    "                label = model.names[int(box.cls[0])]\n",
    "                target_dir = os.path.join(OUTPUT_BASE, \"Web_General\", \"Web\", label)\n",
    "                os.makedirs(target_dir, exist_ok=True)\n",
    "                \n",
    "                x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "                crop = img[y1:y2, x1:x2]\n",
    "                if crop is not None and crop.size > 0:\n",
    "                    cv2.imwrite(os.path.join(target_dir, f\"w_{img_name}_{i}.jpg\"), crop)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_mobile()\n",
    "    process_web()\n",
    "    print(\"---  HARVEST COMPLETE! ---\")\n",
    "\n",
    "\n",
    "    #This code prepares your dataset by detecting, cropping, and organizing UI elements from mobile and web screenshots so that later they can be scored for visual quality and style  saved to Expert_Library ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ea3e814",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'faiss' has no attribute 'StandardGpuResources'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfaiss\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m res = \u001b[43mfaiss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mStandardGpuResources\u001b[49m()  \n",
      "\u001b[31mAttributeError\u001b[39m: module 'faiss' has no attribute 'StandardGpuResources'"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "res = faiss.StandardGpuResources()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a6576f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Starting Style Vectorization ---\n",
      " No images found in Expert_Library! Did the harvest fail?\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "import os\n",
    "import faiss\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 1. Load CLIP \"Style Judge\" (Using GPU for the math, even if FAISS is CPU)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "\n",
    "EXPERT_DIR = \"Expert_Library\"\n",
    "index_file = \"expert_style_index.bin\"\n",
    "paths_file = \"image_paths.txt\"\n",
    "\n",
    "def build_style_index():\n",
    "    all_embeddings = [] # will store CLIP vectors for all expert images\n",
    "    image_paths = [] # will store the file paths\n",
    "    \n",
    "    # Walks through every folder in Expert_Library,Collects all image file paths,Prepare the list of images to convert to vectors\n",
    "    print(\" Starting Style Vectorization ---\")\n",
    "    for root, dirs, files in os.walk(EXPERT_DIR):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                image_paths.append(os.path.join(root, file))\n",
    "\n",
    "    if not image_paths:\n",
    "        print(\" No images found in Expert_Library! Did the harvest fail?\")\n",
    "        return\n",
    "\n",
    "    # Process in batches for GPU efficiency\n",
    "    batch_size = 64\n",
    "    for i in tqdm(range(0, len(image_paths), batch_size)):\n",
    "        batch_paths = image_paths[i:i+batch_size]\n",
    "        batch_imgs = []\n",
    "        \n",
    "        #Preprocess and convert images to CLIP vectors\n",
    "        for p in batch_paths:\n",
    "            try:\n",
    "                img = preprocess(Image.open(p)).unsqueeze(0)\n",
    "                batch_imgs.append(img)\n",
    "            except:\n",
    "                continue\n",
    "            \n",
    "        if not batch_imgs: continue\n",
    "\n",
    "        #Encode images with CLIP\n",
    "        with torch.no_grad():\n",
    "            img_stack = torch.cat(batch_imgs).to(device)\n",
    "            embeddings = model.encode_image(img_stack)\n",
    "            embeddings /= embeddings.norm(dim=-1, keepdim=True)\n",
    "            all_embeddings.append(embeddings.cpu().numpy())\n",
    "\n",
    "    # Create FAISS Index\n",
    "    embeddings_np = np.vstack(all_embeddings).astype('float32')\n",
    "    dimension = embeddings_np.shape[1] # 512 for CLIP\n",
    "    \n",
    "    # index is standard for similarity\n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "    index.add(embeddings_np)\n",
    "    \n",
    "    # 4. Save to Disk\n",
    "    faiss.write_index(index, index_file)\n",
    "    with open(paths_file, \"w\") as f:\n",
    "        for p in image_paths:\n",
    "            f.write(p + \"\\n\")\n",
    "            \n",
    "    print(f\" Brain Built! Indexed {len(image_paths)} professional components into '{index_file}'.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    build_style_index()\n",
    "\n",
    "    \n",
    "\n",
    "# Summary:\n",
    "\n",
    "# Collects all expert UI images\n",
    "\n",
    "# Converts them into numeric vectors using CLIP\n",
    "\n",
    "# Stores vectors in FAISS index for fast similarity search\n",
    "\n",
    "# Saves paths to know which vector belongs to which image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bad92a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Audit Finished. Overall Similarity: 86.1%\n",
      " Marked Image Saved: final_audit_report.jpg\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import clip\n",
    "import faiss\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# --- Load Models & Index ---\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "yolo_model = YOLO(\"best.pt\").to(device)\n",
    "clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "index = faiss.read_index(\"expert_style_index.bin\")\n",
    "\n",
    "def generate_final_audit(image_path, output_path=\"final_audit_report.jpg\"):\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None: return print(\" Image not found\")\n",
    "\n",
    "    # 1. Detection\n",
    "    results = yolo_model.predict(image_path, conf=0.4, imgsz=1024, verbose=False)\n",
    "    scores = []\n",
    "\n",
    "    # 2. Component Analysis\n",
    "    for r in results:\n",
    "        for box in r.boxes:\n",
    "            x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "            label = yolo_model.names[int(box.cls[0])]\n",
    "            \n",
    "            # Crop & Style Vector\n",
    "            crop = img[y1:y2, x1:x2]\n",
    "            if crop.size == 0: continue\n",
    "            pil_crop = Image.fromarray(cv2.cvtColor(crop, cv2.COLOR_BGR2RGB))\n",
    "            img_input = clip_preprocess(pil_crop).unsqueeze(0).to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                user_vec = clip_model.encode_image(img_input)\n",
    "                user_vec /= user_vec.norm(dim=-1, keepdim=True)\n",
    "                \n",
    "            # Search Similarity\n",
    "            distances, _ = index.search(user_vec.cpu().numpy().astype('float32'), 1)\n",
    "            sim_score = max(0, min(100, 100 - (distances[0][0] * 45)))\n",
    "            scores.append(sim_score)\n",
    "\n",
    "            # Draw Detection Box & Label\n",
    "            color = (0, 255, 0) if sim_score > 75 else (0, 255, 255) if sim_score > 50 else (0, 0, 255)\n",
    "            cv2.rectangle(img, (x1, y1), (x2, y2), color, 3)\n",
    "            cv2.putText(img, f\"{label}: {sim_score:.1f}%\", (x1, y1 - 10), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "\n",
    "    # 3. Final Overall Scoring\n",
    "    overall_avg = np.mean(scores) if scores else 0\n",
    "    \n",
    "    # Create Black Header Bar\n",
    "    overlay = img.copy()\n",
    "    cv2.rectangle(overlay, (0, 0), (img.shape[1], 80), (0, 0, 0), -1)\n",
    "    cv2.addWeighted(overlay, 0.7, img, 0.3, 0, img)\n",
    "    \n",
    "    # Burn Overall Score into Image\n",
    "    status = \"EXCELLENT\" if overall_avg > 85 else \"GOOD\" if overall_avg > 70 else \"NEEDS WORK\"\n",
    "    header_text = f\"OVERALL SIMILARITY SCORE: {overall_avg:.1f}% ({status})\"\n",
    "    cv2.putText(img, header_text, (30, 50), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (255, 255, 255), 3)\n",
    "\n",
    "    # Save Result\n",
    "    cv2.imwrite(output_path, img)\n",
    "    print(f\" Audit Finished. Overall Similarity: {overall_avg:.1f}%\")\n",
    "    print(f\" Marked Image Saved: {output_path}\")\n",
    "\n",
    "# --- Run ---\n",
    "generate_final_audit(\"test_ui.png\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dff3eae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c7450a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
