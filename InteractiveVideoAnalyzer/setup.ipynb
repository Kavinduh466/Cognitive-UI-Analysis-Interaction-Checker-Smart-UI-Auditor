{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98b7f900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu124\n",
      "Collecting torch\n",
      "  Obtaining dependency information for torch from https://download.pytorch.org/whl/cu124/torch-2.6.0%2Bcu124-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading https://download.pytorch.org/whl/cu124/torch-2.6.0%2Bcu124-cp311-cp311-win_amd64.whl.metadata (28 kB)\n",
      "Collecting torchvision\n",
      "  Obtaining dependency information for torchvision from https://download.pytorch.org/whl/cu124/torchvision-0.21.0%2Bcu124-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading https://download.pytorch.org/whl/cu124/torchvision-0.21.0%2Bcu124-cp311-cp311-win_amd64.whl.metadata (6.3 kB)\n",
      "Collecting torchaudio\n",
      "  Obtaining dependency information for torchaudio from https://download.pytorch.org/whl/cu124/torchaudio-2.6.0%2Bcu124-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading https://download.pytorch.org/whl/cu124/torchaudio-2.6.0%2Bcu124-cp311-cp311-win_amd64.whl.metadata (6.8 kB)\n",
      "Collecting filelock\n",
      "  Obtaining dependency information for filelock from https://download.pytorch.org/whl/filelock-3.19.1-py3-none-any.whl.metadata\n",
      "  Downloading https://download.pytorch.org/whl/filelock-3.19.1-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\janak\\onedrive\\desktop\\videoanalyzer\\.venv\\lib\\site-packages (from torch) (4.15.0)\n",
      "Collecting networkx\n",
      "  Obtaining dependency information for networkx from https://download.pytorch.org/whl/networkx-3.5-py3-none-any.whl.metadata\n",
      "  Downloading https://download.pytorch.org/whl/networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jinja2\n",
      "  Obtaining dependency information for jinja2 from https://download.pytorch.org/whl/jinja2-3.1.6-py3-none-any.whl.metadata\n",
      "  Downloading https://download.pytorch.org/whl/jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Discarding https://download.pytorch.org/whl/jinja2-3.1.6-py3-none-any.whl (from https://download.pytorch.org/whl/cu124/jinja2/): Requested jinja2 from https://download.pytorch.org/whl/jinja2-3.1.6-py3-none-any.whl (from torch) has inconsistent Name: expected 'jinja2', but metadata has 'Jinja2'\n",
      "  Obtaining dependency information for jinja2 from https://download.pytorch.org/whl/Jinja2-3.1.4-py3-none-any.whl.metadata\n",
      "  Downloading https://download.pytorch.org/whl/Jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
      "Discarding https://download.pytorch.org/whl/Jinja2-3.1.4-py3-none-any.whl#sha256=bc5dd2abb727a5319567b7a813e6a2e7318c39f4f487cfe6c89c6f9c7d25197d (from https://download.pytorch.org/whl/cu124/jinja2/): Requested jinja2 from https://download.pytorch.org/whl/Jinja2-3.1.4-py3-none-any.whl#sha256=bc5dd2abb727a5319567b7a813e6a2e7318c39f4f487cfe6c89c6f9c7d25197d (from torch) has inconsistent Name: expected 'jinja2', but metadata has 'Jinja2'\n",
      "  Downloading https://download.pytorch.org/whl/Jinja2-3.1.3-py3-none-any.whl (133 kB)\n",
      "     ------------------------------------ 133.2/133.2 kB 492.0 kB/s eta 0:00:00\n",
      "Collecting fsspec\n",
      "  Obtaining dependency information for fsspec from https://download.pytorch.org/whl/fsspec-2025.9.0-py3-none-any.whl.metadata\n",
      "  Downloading https://download.pytorch.org/whl/fsspec-2025.9.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting sympy==1.13.1\n",
      "  Downloading https://download.pytorch.org/whl/sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "     ---------------------------------------- 6.2/6.2 MB 5.0 MB/s eta 0:00:00\n",
      "Collecting mpmath<1.4,>=1.1.0\n",
      "  Downloading https://download.pytorch.org/whl/mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "     -------------------------------------- 536.2/536.2 kB 5.6 MB/s eta 0:00:00\n",
      "Collecting numpy\n",
      "  Obtaining dependency information for numpy from https://download.pytorch.org/whl/numpy-2.3.3-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading https://download.pytorch.org/whl/numpy-2.3.3-cp311-cp311-win_amd64.whl.metadata (60 kB)\n",
      "     ---------------------------------------- 60.9/60.9 kB 3.4 MB/s eta 0:00:00\n",
      "Collecting pillow!=8.3.*,>=5.3.0\n",
      "  Obtaining dependency information for pillow!=8.3.*,>=5.3.0 from https://download.pytorch.org/whl/pillow-11.3.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading https://download.pytorch.org/whl/pillow-11.3.0-cp311-cp311-win_amd64.whl.metadata (9.2 kB)\n",
      "Collecting MarkupSafe>=2.0\n",
      "  Downloading https://download.pytorch.org/whl/MarkupSafe-2.1.5-cp311-cp311-win_amd64.whl (17 kB)\n",
      "Downloading https://download.pytorch.org/whl/cu124/torch-2.6.0%2Bcu124-cp311-cp311-win_amd64.whl (2532.4 MB)\n",
      "   ---------------------------------------- 2.5/2.5 GB 769.2 kB/s eta 0:00:00\n",
      "Downloading https://download.pytorch.org/whl/cu124/torchvision-0.21.0%2Bcu124-cp311-cp311-win_amd64.whl (6.1 MB)\n",
      "   ---------------------------------------- 6.1/6.1 MB 6.4 MB/s eta 0:00:00\n",
      "Downloading https://download.pytorch.org/whl/cu124/torchaudio-2.6.0%2Bcu124-cp311-cp311-win_amd64.whl (4.2 MB)\n",
      "   ---------------------------------------- 4.2/4.2 MB 4.8 MB/s eta 0:00:00\n",
      "Downloading https://download.pytorch.org/whl/pillow-11.3.0-cp311-cp311-win_amd64.whl (7.0 MB)\n",
      "   ---------------------------------------- 7.0/7.0 MB 4.7 MB/s eta 0:00:00\n",
      "Downloading https://download.pytorch.org/whl/filelock-3.19.1-py3-none-any.whl (15 kB)\n",
      "Downloading https://download.pytorch.org/whl/fsspec-2025.9.0-py3-none-any.whl (199 kB)\n",
      "   ---------------------------------------- 199.3/199.3 kB 6.1 MB/s eta 0:00:00\n",
      "Downloading https://download.pytorch.org/whl/networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "   ---------------------------------------- 2.0/2.0 MB 1.3 MB/s eta 0:00:00\n",
      "Downloading https://download.pytorch.org/whl/numpy-2.3.3-cp311-cp311-win_amd64.whl (13.1 MB)\n",
      "   ---------------------------------------- 13.1/13.1 MB 6.5 MB/s eta 0:00:00\n",
      "Using cached https://download.pytorch.org/whl/cu124/torch-2.6.0%2Bcu124-cp311-cp311-win_amd64.whl (2532.4 MB)\n",
      "Using cached https://download.pytorch.org/whl/cu124/torchvision-0.21.0%2Bcu124-cp311-cp311-win_amd64.whl (6.1 MB)\n",
      "Using cached https://download.pytorch.org/whl/cu124/torchaudio-2.6.0%2Bcu124-cp311-cp311-win_amd64.whl (4.2 MB)\n",
      "Using cached https://download.pytorch.org/whl/pillow-11.3.0-cp311-cp311-win_amd64.whl (7.0 MB)\n",
      "Using cached https://download.pytorch.org/whl/filelock-3.19.1-py3-none-any.whl (15 kB)\n",
      "Using cached https://download.pytorch.org/whl/fsspec-2025.9.0-py3-none-any.whl (199 kB)\n",
      "Using cached https://download.pytorch.org/whl/networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "Using cached https://download.pytorch.org/whl/numpy-2.3.3-cp311-cp311-win_amd64.whl (13.1 MB)\n",
      "Installing collected packages: mpmath, sympy, pillow, numpy, networkx, MarkupSafe, fsspec, filelock, jinja2, torch, torchvision, torchaudio\n",
      "Successfully installed MarkupSafe-2.1.5 filelock-3.19.1 fsspec-2025.9.0 jinja2-3.1.3 mpmath-1.3.0 networkx-3.5 numpy-2.3.3 pillow-11.3.0 sympy-1.13.1 torch-2.6.0+cu124 torchaudio-2.6.0+cu124 torchvision-0.21.0+cu124\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14777aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python\n",
      "  Using cached opencv_python-4.12.0.88-cp37-abi3-win_amd64.whl (39.0 MB)\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.3.3-cp311-cp311-win_amd64.whl (11.3 MB)\n",
      "     ---------------------------------------- 11.3/11.3 MB 6.0 MB/s eta 0:00:00\n",
      "Collecting ffmpeg-python\n",
      "  Downloading ffmpeg_python-0.2.0-py3-none-any.whl (25 kB)\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.8-cp311-cp311-win_amd64.whl (8.1 MB)\n",
      "     ---------------------------------------- 8.1/8.1 MB 5.2 MB/s eta 0:00:00\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.8.0-cp311-cp311-win_amd64.whl (8.1 MB)\n",
      "     ---------------------------------------- 8.1/8.1 MB 7.6 MB/s eta 0:00:00\n",
      "Collecting pytorchvideo\n",
      "  Downloading pytorchvideo-0.1.5.tar.gz (132 kB)\n",
      "     -------------------------------------- 132.7/132.7 kB 7.7 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting ipywidgets\n",
      "  Downloading ipywidgets-8.1.8-py3-none-any.whl (139 kB)\n",
      "     -------------------------------------- 139.8/139.8 kB 8.1 MB/s eta 0:00:00\n",
      "Collecting numpy<2.3.0,>=2\n",
      "  Downloading numpy-2.2.6-cp311-cp311-win_amd64.whl (12.9 MB)\n",
      "     ---------------------------------------- 12.9/12.9 MB 8.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\janak\\onedrive\\desktop\\videoanalyzer\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "     ------------------------------------- 509.2/509.2 kB 10.6 MB/s eta 0:00:00\n",
      "Collecting tzdata>=2022.7\n",
      "  Downloading tzdata-2025.3-py2.py3-none-any.whl (348 kB)\n",
      "     ------------------------------------- 348.5/348.5 kB 10.9 MB/s eta 0:00:00\n",
      "Collecting future\n",
      "  Downloading future-1.0.0-py3-none-any.whl (491 kB)\n",
      "     -------------------------------------- 491.3/491.3 kB 7.6 MB/s eta 0:00:00\n",
      "Collecting contourpy>=1.0.1\n",
      "  Downloading contourpy-1.3.3-cp311-cp311-win_amd64.whl (225 kB)\n",
      "     -------------------------------------- 225.2/225.2 kB 4.6 MB/s eta 0:00:00\n",
      "Collecting cycler>=0.10\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.61.1-cp311-cp311-win_amd64.whl (2.3 MB)\n",
      "     ---------------------------------------- 2.3/2.3 MB 8.7 MB/s eta 0:00:00\n",
      "Collecting kiwisolver>=1.3.1\n",
      "  Downloading kiwisolver-1.4.9-cp311-cp311-win_amd64.whl (73 kB)\n",
      "     ---------------------------------------- 73.8/73.8 kB 4.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\janak\\onedrive\\desktop\\videoanalyzer\\.venv\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\janak\\onedrive\\desktop\\videoanalyzer\\.venv\\lib\\site-packages (from matplotlib) (11.3.0)\n",
      "Collecting pyparsing>=3\n",
      "  Downloading pyparsing-3.2.5-py3-none-any.whl (113 kB)\n",
      "     -------------------------------------- 113.9/113.9 kB 3.2 MB/s eta 0:00:00\n",
      "Collecting scipy>=1.10.0\n",
      "  Downloading scipy-1.16.3-cp311-cp311-win_amd64.whl (38.7 MB)\n",
      "     ---------------------------------------- 38.7/38.7 MB 6.0 MB/s eta 0:00:00\n",
      "Collecting joblib>=1.3.0\n",
      "  Downloading joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "     -------------------------------------- 308.4/308.4 kB 9.6 MB/s eta 0:00:00\n",
      "Collecting threadpoolctl>=3.2.0\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Collecting fvcore\n",
      "  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n",
      "     ---------------------------------------- 50.2/50.2 kB ? eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting av\n",
      "  Downloading av-16.0.1-cp311-cp311-win_amd64.whl (32.3 MB)\n",
      "     ---------------------------------------- 32.3/32.3 MB 6.9 MB/s eta 0:00:00\n",
      "Collecting parameterized\n",
      "  Downloading parameterized-0.9.0-py2.py3-none-any.whl (20 kB)\n",
      "Collecting iopath\n",
      "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
      "     ---------------------------------------- 42.2/42.2 kB 2.0 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: networkx in c:\\users\\janak\\onedrive\\desktop\\videoanalyzer\\.venv\\lib\\site-packages (from pytorchvideo) (3.5)\n",
      "Requirement already satisfied: comm>=0.1.3 in c:\\users\\janak\\onedrive\\desktop\\videoanalyzer\\.venv\\lib\\site-packages (from ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\janak\\onedrive\\desktop\\videoanalyzer\\.venv\\lib\\site-packages (from ipywidgets) (9.8.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\janak\\onedrive\\desktop\\videoanalyzer\\.venv\\lib\\site-packages (from ipywidgets) (5.14.3)\n",
      "Collecting widgetsnbextension~=4.0.14\n",
      "  Downloading widgetsnbextension-4.0.15-py3-none-any.whl (2.2 MB)\n",
      "     ---------------------------------------- 2.2/2.2 MB 7.4 MB/s eta 0:00:00\n",
      "Collecting jupyterlab_widgets~=3.0.15\n",
      "  Downloading jupyterlab_widgets-3.0.16-py3-none-any.whl (914 kB)\n",
      "     -------------------------------------- 914.9/914.9 kB 6.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: colorama>=0.4.4 in c:\\users\\janak\\onedrive\\desktop\\videoanalyzer\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.4.6)\n",
      "Requirement already satisfied: decorator>=4.3.2 in c:\\users\\janak\\onedrive\\desktop\\videoanalyzer\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers>=1.0.0 in c:\\users\\janak\\onedrive\\desktop\\videoanalyzer\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.18.1 in c:\\users\\janak\\onedrive\\desktop\\videoanalyzer\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1.5 in c:\\users\\janak\\onedrive\\desktop\\videoanalyzer\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.2.1)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in c:\\users\\janak\\onedrive\\desktop\\videoanalyzer\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.52)\n",
      "Requirement already satisfied: pygments>=2.11.0 in c:\\users\\janak\\onedrive\\desktop\\videoanalyzer\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (2.19.2)\n",
      "Requirement already satisfied: stack_data>=0.6.0 in c:\\users\\janak\\onedrive\\desktop\\videoanalyzer\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: typing_extensions>=4.6 in c:\\users\\janak\\onedrive\\desktop\\videoanalyzer\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (4.15.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\janak\\onedrive\\desktop\\videoanalyzer\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Collecting yacs>=0.1.6\n",
      "  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
      "Collecting pyyaml>=5.1\n",
      "  Downloading pyyaml-6.0.3-cp311-cp311-win_amd64.whl (158 kB)\n",
      "     -------------------------------------- 158.8/158.8 kB 9.9 MB/s eta 0:00:00\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "     ---------------------------------------- 78.5/78.5 kB 4.3 MB/s eta 0:00:00\n",
      "Collecting termcolor>=1.1\n",
      "  Downloading termcolor-3.2.0-py3-none-any.whl (7.7 kB)\n",
      "Collecting tabulate\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Collecting portalocker\n",
      "  Downloading portalocker-3.2.0-py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in c:\\users\\janak\\onedrive\\desktop\\videoanalyzer\\.venv\\lib\\site-packages (from jedi>=0.18.1->ipython>=6.1.0->ipywidgets) (0.8.5)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\janak\\onedrive\\desktop\\videoanalyzer\\.venv\\lib\\site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.14)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\janak\\onedrive\\desktop\\videoanalyzer\\.venv\\lib\\site-packages (from stack_data>=0.6.0->ipython>=6.1.0->ipywidgets) (2.2.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\janak\\onedrive\\desktop\\videoanalyzer\\.venv\\lib\\site-packages (from stack_data>=0.6.0->ipython>=6.1.0->ipywidgets) (3.0.1)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\janak\\onedrive\\desktop\\videoanalyzer\\.venv\\lib\\site-packages (from stack_data>=0.6.0->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Collecting pywin32>=226\n",
      "  Downloading pywin32-311-cp311-cp311-win_amd64.whl (9.5 MB)\n",
      "     ---------------------------------------- 9.5/9.5 MB 7.0 MB/s eta 0:00:00\n",
      "Installing collected packages: pywin32, pytz, widgetsnbextension, tzdata, tqdm, threadpoolctl, termcolor, tabulate, pyyaml, pyparsing, portalocker, parameterized, numpy, kiwisolver, jupyterlab_widgets, joblib, future, fonttools, cycler, av, yacs, scipy, pandas, opencv-python, iopath, ffmpeg-python, contourpy, scikit-learn, matplotlib, ipywidgets, fvcore, pytorchvideo\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.3.3\n",
      "    Uninstalling numpy-2.3.3:\n",
      "      Successfully uninstalled numpy-2.3.3\n",
      "  Running setup.py install for iopath: started\n",
      "  Running setup.py install for iopath: finished with status 'done'\n",
      "  Running setup.py install for fvcore: started\n",
      "  Running setup.py install for fvcore: finished with status 'done'\n",
      "  Running setup.py install for pytorchvideo: started\n",
      "  Running setup.py install for pytorchvideo: finished with status 'done'\n",
      "Successfully installed av-16.0.1 contourpy-1.3.3 cycler-0.12.1 ffmpeg-python-0.2.0 fonttools-4.61.1 future-1.0.0 fvcore-0.1.5.post20221221 iopath-0.1.10 ipywidgets-8.1.8 joblib-1.5.2 jupyterlab_widgets-3.0.16 kiwisolver-1.4.9 matplotlib-3.10.8 numpy-2.2.6 opencv-python-4.12.0.88 pandas-2.3.3 parameterized-0.9.0 portalocker-3.2.0 pyparsing-3.2.5 pytorchvideo-0.1.5 pytz-2025.2 pywin32-311 pyyaml-6.0.3 scikit-learn-1.8.0 scipy-1.16.3 tabulate-0.9.0 termcolor-3.2.0 threadpoolctl-3.6.0 tqdm-4.67.1 tzdata-2025.3 widgetsnbextension-4.0.15 yacs-0.1.8\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  DEPRECATION: iopath is being installed using the legacy 'setup.py install' method, because it does not have a 'pyproject.toml' and the 'wheel' package is not installed. pip 23.1 will enforce this behaviour change. A possible replacement is to enable the '--use-pep517' option. Discussion can be found at https://github.com/pypa/pip/issues/8559\n",
      "  DEPRECATION: fvcore is being installed using the legacy 'setup.py install' method, because it does not have a 'pyproject.toml' and the 'wheel' package is not installed. pip 23.1 will enforce this behaviour change. A possible replacement is to enable the '--use-pep517' option. Discussion can be found at https://github.com/pypa/pip/issues/8559\n",
      "  DEPRECATION: pytorchvideo is being installed using the legacy 'setup.py install' method, because it does not have a 'pyproject.toml' and the 'wheel' package is not installed. pip 23.1 will enforce this behaviour change. A possible replacement is to enable the '--use-pep517' option. Discussion can be found at https://github.com/pypa/pip/issues/8559\n",
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    }
   ],
   "source": [
    "%pip install opencv-python pandas ffmpeg-python matplotlib scikit-learn pytorchvideo ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de8ba5ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- System Check ---\n",
      "PyTorch Version: 2.6.0+cu124\n",
      "CUDA Available:  False\n",
      "WARNING: GPU not found. Training will be slow.\n",
      "OpenCV Version:  4.11.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import ffmpeg\n",
    "\n",
    "print(\"--- System Check ---\")\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"CUDA Available:  {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Name:        {torch.cuda.get_device_name(0)}\")\n",
    "    print(\"SUCCESS: Your RTX 4070 is ready for training!\")\n",
    "else:\n",
    "    print(\"WARNING: GPU not found. Training will be slow.\")\n",
    "\n",
    "print(f\"OpenCV Version:  {cv2.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b15f266e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning folder: dataset...\n",
      "\n",
      "Found 103 cam videos and 102 screen videos.\n",
      "------------------------------------------------------------\n",
      "ID    | Cam Start            | Screen Start         | Offset (s)\n",
      "------------------------------------------------------------\n",
      "1     | Error/No Meta        | Error/No Meta        | N/A\n",
      "2     | Error/No Meta        | Error/No Meta        | N/A\n",
      "3     | Error/No Meta        | Error/No Meta        | N/A\n",
      "4     | Error/No Meta        | Error/No Meta        | N/A\n",
      "5     | Error/No Meta        | Error/No Meta        | N/A\n",
      "6     | Error/No Meta        | Error/No Meta        | N/A\n",
      "7     | Error/No Meta        | Error/No Meta        | N/A\n",
      "8     | Error/No Meta        | Error/No Meta        | N/A\n",
      "9     | Error/No Meta        | Error/No Meta        | N/A\n",
      "10    | Error/No Meta        | Error/No Meta        | N/A\n",
      "11    | Error/No Meta        | Error/No Meta        | N/A\n",
      "12    | Error/No Meta        | Error/No Meta        | N/A\n",
      "13    | Error/No Meta        | Error/No Meta        | N/A\n",
      "14    | Error/No Meta        | Error/No Meta        | N/A\n",
      "15    | Error/No Meta        | Error/No Meta        | N/A\n",
      "16    | Error/No Meta        | Error/No Meta        | N/A\n",
      "17    | Error/No Meta        | Error/No Meta        | N/A\n",
      "18    | Error/No Meta        | Error/No Meta        | N/A\n",
      "19    | Error/No Meta        | Error/No Meta        | N/A\n",
      "20    | Error/No Meta        | Error/No Meta        | N/A\n",
      "21    | Error/No Meta        | Error/No Meta        | N/A\n",
      "22    | Error/No Meta        | Error/No Meta        | N/A\n",
      "23    | Error/No Meta        | Error/No Meta        | N/A\n",
      "24    | Error/No Meta        | Error/No Meta        | N/A\n",
      "25    | Error/No Meta        | Error/No Meta        | N/A\n",
      "26    | Error/No Meta        | Error/No Meta        | N/A\n",
      "27    | Error/No Meta        | Error/No Meta        | N/A\n",
      "28    | Error/No Meta        | Error/No Meta        | N/A\n",
      "29    | Error/No Meta        | Error/No Meta        | N/A\n",
      "30    | Error/No Meta        | Error/No Meta        | N/A\n",
      "31    | Error/No Meta        | Error/No Meta        | N/A\n",
      "32    | Error/No Meta        | Error/No Meta        | N/A\n",
      "33    | Error/No Meta        | Error/No Meta        | N/A\n",
      "34    | Error/No Meta        | Error/No Meta        | N/A\n",
      "35    | Error/No Meta        | Error/No Meta        | N/A\n",
      "36    | Error/No Meta        | Error/No Meta        | N/A\n",
      "37    | Error/No Meta        | Error/No Meta        | N/A\n",
      "38    | Error/No Meta        | Error/No Meta        | N/A\n",
      "39    | Error/No Meta        | Error/No Meta        | N/A\n",
      "40    | Error/No Meta        | Error/No Meta        | N/A\n",
      "41    | Error/No Meta        | Error/No Meta        | N/A\n",
      "42    | Error/No Meta        | Error/No Meta        | N/A\n",
      "43    | Error/No Meta        | Error/No Meta        | N/A\n",
      "44    | Error/No Meta        | Error/No Meta        | N/A\n",
      "45    | Error/No Meta        | Error/No Meta        | N/A\n",
      "46    | Error/No Meta        | Error/No Meta        | N/A\n",
      "47    | Error/No Meta        | Error/No Meta        | N/A\n",
      "48    | Error/No Meta        | Error/No Meta        | N/A\n",
      "49    | Error/No Meta        | Error/No Meta        | N/A\n",
      "50    | Error/No Meta        | Error/No Meta        | N/A\n",
      "51    | Error/No Meta        | Error/No Meta        | N/A\n",
      "52    | Error/No Meta        | Error/No Meta        | N/A\n",
      "53    | Error/No Meta        | Error/No Meta        | N/A\n",
      "54    | Error/No Meta        | Error/No Meta        | N/A\n",
      "55    | Error/No Meta        | Error/No Meta        | N/A\n",
      "57    | Error/No Meta        | Error/No Meta        | N/A\n",
      "58    | Error/No Meta        | Error/No Meta        | N/A\n",
      "59    | Error/No Meta        | Error/No Meta        | N/A\n",
      "60    | Error/No Meta        | Error/No Meta        | N/A\n",
      "61    | Error/No Meta        | Error/No Meta        | N/A\n",
      "62    | Error/No Meta        | Error/No Meta        | N/A\n",
      "63    | Error/No Meta        | Error/No Meta        | N/A\n",
      "64    | Error/No Meta        | Error/No Meta        | N/A\n",
      "65    | Error/No Meta        | Error/No Meta        | N/A\n",
      "66    | Error/No Meta        | Error/No Meta        | N/A\n",
      "67    | Error/No Meta        | Error/No Meta        | N/A\n",
      "68    | Error/No Meta        | Error/No Meta        | N/A\n",
      "69    | Error/No Meta        | Error/No Meta        | N/A\n",
      "70    | Error/No Meta        | Error/No Meta        | N/A\n",
      "71    | Error/No Meta        | Error/No Meta        | N/A\n",
      "72    | Error/No Meta        | Error/No Meta        | N/A\n",
      "73    | Error/No Meta        | Error/No Meta        | N/A\n",
      "74    | Error/No Meta        | Error/No Meta        | N/A\n",
      "75    | Error/No Meta        | Error/No Meta        | N/A\n",
      "76    | Error/No Meta        | Error/No Meta        | N/A\n",
      "77    | Error/No Meta        | Error/No Meta        | N/A\n",
      "78    | Error/No Meta        | Error/No Meta        | N/A\n",
      "79    | Error/No Meta        | Error/No Meta        | N/A\n",
      "80    | Error/No Meta        | Error/No Meta        | N/A\n",
      "81    | Error/No Meta        | Error/No Meta        | N/A\n",
      "82    | Error/No Meta        | Error/No Meta        | N/A\n",
      "83    | Error/No Meta        | Error/No Meta        | N/A\n",
      "84    | Error/No Meta        | Error/No Meta        | N/A\n",
      "85    | Error/No Meta        | Error/No Meta        | N/A\n",
      "86    | Error/No Meta        | Error/No Meta        | N/A\n",
      "87    | Error/No Meta        | Error/No Meta        | N/A\n",
      "88    | Error/No Meta        | Error/No Meta        | N/A\n",
      "89    | Error/No Meta        | Error/No Meta        | N/A\n",
      "90    | Error/No Meta        | Error/No Meta        | N/A\n",
      "91    | Error/No Meta        | Error/No Meta        | N/A\n",
      "92    | Error/No Meta        | Error/No Meta        | N/A\n",
      "93    | Error/No Meta        | Error/No Meta        | N/A\n",
      "94    | Error/No Meta        | Error/No Meta        | N/A\n",
      "95    | Error/No Meta        | Error/No Meta        | N/A\n",
      "96    | Error/No Meta        | Error/No Meta        | N/A\n",
      "97    | Error/No Meta        | Error/No Meta        | N/A\n",
      "98    | Error/No Meta        | Error/No Meta        | N/A\n",
      "99    | Error/No Meta        | Error/No Meta        | N/A\n",
      "100   | Error/No Meta        | Error/No Meta        | N/A\n",
      "101   | Error/No Meta        | Error/No Meta        | N/A\n",
      "102   | Error/No Meta        | Error/No Meta        | N/A\n",
      "103   | Error/No Meta        | Error/No Meta        | N/A\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import subprocess\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "\n",
    "VIDEO_FOLDER = \"dataset\" \n",
    "\n",
    "\n",
    "def get_creation_time(filepath):\n",
    "    \"\"\"Extracts creation_time from video metadata using ffprobe.\"\"\"\n",
    "    try:\n",
    "        cmd = [\n",
    "            'ffprobe', '-v', 'quiet', '-print_format', 'json',\n",
    "            '-show_entries', 'format_tags=creation_time', \n",
    "            filepath\n",
    "        ]\n",
    "        # We use shell=True only if on Windows and commands aren't found, \n",
    "        # but since you added to PATH, it should work without it.\n",
    "        result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "        data = json.loads(result.stdout)\n",
    "        tags = data.get('format', {}).get('tags', {})\n",
    "        time_str = tags.get('creation_time')\n",
    "        \n",
    "        if time_str:\n",
    "            # Parse ISO 8601 format (e.g., 2025-10-24T14:05:01.000000Z)\n",
    "            # We treat it as UTC\n",
    "            return datetime.fromisoformat(time_str.replace('Z', '+00:00'))\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {filepath}: {e}\")\n",
    "        return None\n",
    "\n",
    "def parse_video_id(filename):\n",
    "    \"\"\"Extracts the number from 'cam1.mp4' or 'Cam34.mp4'\"\"\"\n",
    "    # Find all digits in the filename\n",
    "    match = re.search(r'(\\d+)', filename)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    return None\n",
    "\n",
    "# 1. Scan and Pair Files\n",
    "cam_files = {}\n",
    "screen_files = {}\n",
    "\n",
    "print(f\"Scanning folder: {VIDEO_FOLDER}...\")\n",
    "\n",
    "if not os.path.exists(VIDEO_FOLDER):\n",
    "    print(f\"ERROR: Folder '{VIDEO_FOLDER}' does not exist.\")\n",
    "else:\n",
    "    for f in os.listdir(VIDEO_FOLDER):\n",
    "        if not f.endswith(('.mp4', '.mkv', '.mov', '.avi')):\n",
    "            continue\n",
    "            \n",
    "        vid_id = parse_video_id(f)\n",
    "        if vid_id is None:\n",
    "            continue\n",
    "            \n",
    "        full_path = os.path.join(VIDEO_FOLDER, f)\n",
    "        \n",
    "        # Check if it is a cam or screen video (case insensitive check)\n",
    "        lower_name = f.lower()\n",
    "        if \"cam\" in lower_name:\n",
    "            cam_files[vid_id] = full_path\n",
    "        elif \"screen\" in lower_name:\n",
    "            screen_files[vid_id] = full_path\n",
    "\n",
    "    # 2. Analyze Pairs\n",
    "    print(f\"\\nFound {len(cam_files)} cam videos and {len(screen_files)} screen videos.\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"{'ID':<5} | {'Cam Start':<20} | {'Screen Start':<20} | {'Offset (s)':<10}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    common_ids = sorted(list(set(cam_files.keys()) & set(screen_files.keys())))\n",
    "\n",
    "    for i in common_ids:\n",
    "        c_path = cam_files[i]\n",
    "        s_path = screen_files[i]\n",
    "        \n",
    "        t_cam = get_creation_time(c_path)\n",
    "        t_screen = get_creation_time(s_path)\n",
    "        \n",
    "        if t_cam and t_screen:\n",
    "            # Calculate difference in seconds\n",
    "            diff = (t_cam - t_screen).total_seconds()\n",
    "            \n",
    "            # Formatting for display\n",
    "            t_c_str = t_cam.strftime('%H:%M:%S')\n",
    "            t_s_str = t_screen.strftime('%H:%M:%S')\n",
    "            \n",
    "            print(f\"{i:<5} | {t_c_str:<20} | {t_s_str:<20} | {diff:.2f}\")\n",
    "        else:\n",
    "            print(f\"{i:<5} | {'Error/No Meta':<20} | {'Error/No Meta':<20} | N/A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73b2afd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Audio Check ---\n",
      "Cam1 Audio:    YES (Audio Found: aac)\n",
      "Screen1 Audio: YES (Audio Found: aac)\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import json\n",
    "import os\n",
    "\n",
    "# CONFIGURATION\n",
    "# Pick one pair that you think might have audio\n",
    "cam_sample = \"dataset/cam1.mp4\" \n",
    "screen_sample = \"dataset/screen1.mp4\"\n",
    "\n",
    "def check_audio_stream(filepath):\n",
    "    if not os.path.exists(filepath):\n",
    "        return \"File not found\"\n",
    "        \n",
    "    try:\n",
    "        # Ask ffprobe for stream information\n",
    "        cmd = [\n",
    "            'ffprobe', '-v', 'quiet', '-print_format', 'json',\n",
    "            '-show_streams', '-select_streams', 'a',  # 'a' looks for Audio only\n",
    "            filepath\n",
    "        ]\n",
    "        result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "        data = json.loads(result.stdout)\n",
    "        \n",
    "        streams = data.get('streams', [])\n",
    "        if len(streams) > 0:\n",
    "            return f\"YES (Audio Found: {streams[0].get('codec_name')})\"\n",
    "        else:\n",
    "            return \"NO Audio Stream\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "print(f\"--- Audio Check ---\")\n",
    "print(f\"Cam1 Audio:    {check_audio_stream(cam_sample)}\")\n",
    "print(f\"Screen1 Audio: {check_audio_stream(screen_sample)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cdd523b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting librosa\n",
      "  Downloading librosa-0.11.0-py3-none-any.whl (260 kB)\n",
      "     -------------------------------------- 260.7/260.7 kB 1.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: scipy in c:\\users\\janak\\onedrive\\desktop\\videoanalyzer\\.venv\\lib\\site-packages (1.16.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\janak\\onedrive\\desktop\\videoanalyzer\\.venv\\lib\\site-packages (2.2.6)\n",
      "Collecting audioread>=2.1.9\n",
      "  Downloading audioread-3.1.0-py3-none-any.whl (23 kB)\n",
      "Collecting numba>=0.51.0\n",
      "  Downloading numba-0.63.1-cp311-cp311-win_amd64.whl (2.7 MB)\n",
      "     ---------------------------------------- 2.7/2.7 MB 1.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: scikit-learn>=1.1.0 in c:\\users\\janak\\onedrive\\desktop\\videoanalyzer\\.venv\\lib\\site-packages (from librosa) (1.8.0)\n",
      "Requirement already satisfied: joblib>=1.0 in c:\\users\\janak\\onedrive\\desktop\\videoanalyzer\\.venv\\lib\\site-packages (from librosa) (1.5.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in c:\\users\\janak\\onedrive\\desktop\\videoanalyzer\\.venv\\lib\\site-packages (from librosa) (5.2.1)\n",
      "Collecting soundfile>=0.12.1\n",
      "  Downloading soundfile-0.13.1-py2.py3-none-win_amd64.whl (1.0 MB)\n",
      "     ---------------------------------------- 1.0/1.0 MB 1.8 MB/s eta 0:00:00\n",
      "Collecting pooch>=1.1\n",
      "  Downloading pooch-1.8.2-py3-none-any.whl (64 kB)\n",
      "     ---------------------------------------- 64.6/64.6 kB 3.4 MB/s eta 0:00:00\n",
      "Collecting soxr>=0.3.2\n",
      "  Downloading soxr-1.0.0-cp311-cp311-win_amd64.whl (173 kB)\n",
      "     -------------------------------------- 173.8/173.8 kB 2.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: typing_extensions>=4.1.1 in c:\\users\\janak\\onedrive\\desktop\\videoanalyzer\\.venv\\lib\\site-packages (from librosa) (4.15.0)\n",
      "Collecting lazy_loader>=0.1\n",
      "  Downloading lazy_loader-0.4-py3-none-any.whl (12 kB)\n",
      "Collecting msgpack>=1.0\n",
      "  Downloading msgpack-1.1.2-cp311-cp311-win_amd64.whl (71 kB)\n",
      "     ---------------------------------------- 71.6/71.6 kB 3.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging in c:\\users\\janak\\onedrive\\desktop\\videoanalyzer\\.venv\\lib\\site-packages (from lazy_loader>=0.1->librosa) (25.0)\n",
      "Collecting llvmlite<0.47,>=0.46.0dev0\n",
      "  Downloading llvmlite-0.46.0-cp311-cp311-win_amd64.whl (38.1 MB)\n",
      "     ---------------------------------------- 38.1/38.1 MB 2.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in c:\\users\\janak\\onedrive\\desktop\\videoanalyzer\\.venv\\lib\\site-packages (from pooch>=1.1->librosa) (4.5.1)\n",
      "Collecting requests>=2.19.0\n",
      "  Downloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "     ---------------------------------------- 64.7/64.7 kB 1.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: threadpoolctl>=3.2.0 in c:\\users\\janak\\onedrive\\desktop\\videoanalyzer\\.venv\\lib\\site-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\n",
      "Collecting cffi>=1.0\n",
      "  Downloading cffi-2.0.0-cp311-cp311-win_amd64.whl (182 kB)\n",
      "     -------------------------------------- 182.8/182.8 kB 3.7 MB/s eta 0:00:00\n",
      "Collecting pycparser\n",
      "  Downloading pycparser-2.23-py3-none-any.whl (118 kB)\n",
      "     -------------------------------------- 118.1/118.1 kB 3.5 MB/s eta 0:00:00\n",
      "Collecting charset_normalizer<4,>=2\n",
      "  Downloading charset_normalizer-3.4.4-cp311-cp311-win_amd64.whl (106 kB)\n",
      "     -------------------------------------- 107.0/107.0 kB 3.0 MB/s eta 0:00:00\n",
      "Collecting idna<4,>=2.5\n",
      "  Downloading idna-3.11-py3-none-any.whl (71 kB)\n",
      "     ---------------------------------------- 71.0/71.0 kB 4.1 MB/s eta 0:00:00\n",
      "Collecting urllib3<3,>=1.21.1\n",
      "  Downloading urllib3-2.6.2-py3-none-any.whl (131 kB)\n",
      "     -------------------------------------- 131.2/131.2 kB 7.6 MB/s eta 0:00:00\n",
      "Collecting certifi>=2017.4.17\n",
      "  Downloading certifi-2025.11.12-py3-none-any.whl (159 kB)\n",
      "     -------------------------------------- 159.4/159.4 kB 4.8 MB/s eta 0:00:00\n",
      "Installing collected packages: urllib3, soxr, pycparser, msgpack, llvmlite, lazy_loader, idna, charset_normalizer, certifi, audioread, requests, numba, cffi, soundfile, pooch, librosa\n",
      "Successfully installed audioread-3.1.0 certifi-2025.11.12 cffi-2.0.0 charset_normalizer-3.4.4 idna-3.11 lazy_loader-0.4 librosa-0.11.0 llvmlite-0.46.0 msgpack-1.1.2 numba-0.63.1 pooch-1.8.2 pycparser-2.23 requests-2.32.5 soundfile-0.13.1 soxr-1.0.0 urllib3-2.6.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    }
   ],
   "source": [
    "%pip install librosa scipy numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3497fdca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading audio from dataset/cam1.mp4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\janak\\AppData\\Local\\Temp\\ipykernel_27192\\1469134873.py:15: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  y1, sr1 = librosa.load(file1, duration=duration, mono=True)\n",
      "c:\\Users\\janak\\OneDrive\\Desktop\\VideoAnalyzer\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
      "C:\\Users\\janak\\AppData\\Local\\Temp\\ipykernel_27192\\1469134873.py:16: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  y2, sr2 = librosa.load(file2, duration=duration, mono=True)\n",
      "c:\\Users\\janak\\OneDrive\\Desktop\\VideoAnalyzer\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio loaded. Calculating synchronization...\n",
      "----------------------------------------\n",
      "Calculated Offset: -60.0000 seconds\n",
      "----------------------------------------\n",
      "INTERPRETATION: 'Screen' starts 60.00s BEFORE 'Cam'.\n",
      "We need to trim the start of Screen.\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "import os\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "FILE_CAM = \"dataset/cam1.mp4\"\n",
    "FILE_SCREEN = \"dataset/screen1.mp4\"\n",
    "# ---------------------\n",
    "\n",
    "def find_audio_offset(file1, file2, duration=60):\n",
    "    print(f\"Loading audio from {file1}...\")\n",
    "    # Load audio (mono, original sample rate)\n",
    "    try:\n",
    "        y1, sr1 = librosa.load(file1, duration=duration, mono=True)\n",
    "        y2, sr2 = librosa.load(file2, duration=duration, mono=True)\n",
    "    except Exception as e:\n",
    "        return f\"Error loading audio: {str(e)}\"\n",
    "\n",
    "    print(\"Audio loaded. Calculating synchronization...\")\n",
    "    \n",
    "    # Resample if rates are different (rare, but good safety)\n",
    "    if sr1 != sr2:\n",
    "        print(\"Resampling audio to match...\")\n",
    "        # We assume sr1 is the target\n",
    "        y2 = librosa.resample(y2, orig_sr=sr2, target_sr=sr1)\n",
    "        sr = sr1\n",
    "    else:\n",
    "        sr = sr1\n",
    "\n",
    "    # Cross-correlation (The Math Magic)\n",
    "    # This finds the point where the two signals are most similar\n",
    "    correlation = signal.correlate(y1, y2, mode='full')\n",
    "    lags = signal.correlation_lags(len(y1), len(y2), mode='full')\n",
    "    \n",
    "    # Find the peak correlation\n",
    "    peak_idx = np.argmax(correlation)\n",
    "    lag_samples = lags[peak_idx]\n",
    "    \n",
    "    # Convert samples to seconds\n",
    "    offset_seconds = lag_samples / sr\n",
    "    \n",
    "    return offset_seconds\n",
    "\n",
    "if not os.path.exists(FILE_CAM) or not os.path.exists(FILE_SCREEN):\n",
    "    print(\"Error: Check your filenames. I can't find cam1 or screen1.\")\n",
    "else:\n",
    "    offset = find_audio_offset(FILE_CAM, FILE_SCREEN)\n",
    "    \n",
    "    print(\"-\" * 40)\n",
    "    print(f\"Calculated Offset: {offset:.4f} seconds\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    if offset > 0:\n",
    "        print(f\"INTERPRETATION: 'Cam' starts {offset:.2f}s BEFORE 'Screen'.\")\n",
    "        print(\"We need to trim the start of Cam.\")\n",
    "    elif offset < 0:\n",
    "        print(f\"INTERPRETATION: 'Screen' starts {abs(offset):.2f}s BEFORE 'Cam'.\")\n",
    "        print(\"We need to trim the start of Screen.\")\n",
    "    else:\n",
    "        print(\"Perfectly synced (or audio was silent).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3453e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\janak\\AppData\\Local\\Temp\\ipykernel_27192\\2377665757.py:14: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  y1, sr1 = librosa.load(FILE_CAM, duration=30, mono=True)\n",
      "c:\\Users\\janak\\OneDrive\\Desktop\\VideoAnalyzer\\.venv\\Lib\\site-packages\\librosa\\core\\audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
      "C:\\Users\\janak\\AppData\\Local\\Temp\\ipykernel_27192\\2377665757.py:25: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  y2, sr2 = librosa.load(FILE_SCREEN, duration=30, mono=True)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "FILE_CAM = \"dataset/cam1.mp4\"\n",
    "FILE_SCREEN = \"dataset/screen1.mp4\"\n",
    "# ---------------------\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Load and Plot Cam Audio\n",
    "try:\n",
    "    y1, sr1 = librosa.load(FILE_CAM, duration=30, mono=True)\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(np.linspace(0, 30, len(y1)), y1, color='blue')\n",
    "    plt.title(f\"Cam Audio (First 30s) - {FILE_CAM}\")\n",
    "    plt.ylabel(\"Amplitude\")\n",
    "    plt.ylim(-1, 1)\n",
    "except Exception as e:\n",
    "    print(f\"Could not load Cam audio: {e}\")\n",
    "\n",
    "# Load and Plot Screen Audio\n",
    "try:\n",
    "    y2, sr2 = librosa.load(FILE_SCREEN, duration=30, mono=True)\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(np.linspace(0, 30, len(y2)), y2, color='orange')\n",
    "    plt.title(f\"Screen Audio (First 30s) - {FILE_SCREEN}\")\n",
    "    plt.xlabel(\"Time (seconds)\")\n",
    "    plt.ylabel(\"Amplitude\")\n",
    "    plt.ylim(-1, 1)\n",
    "except Exception as e:\n",
    "    print(f\"Could not load Screen audio: {e}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "535f161b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning dataset to build inventory...\n",
      "--------------------------------------------------\n",
      "Inventory saved to video_inventory.csv\n",
      "Total Valid Pairs: 102\n",
      "--------------------------------------------------\n",
      "   id          cam_path          screen_path  cam_fps  screen_fps  \\\n",
      "0   1  dataset\\cam1.mp4  dataset\\screen1.mp4     60.0        60.0   \n",
      "1   2  dataset\\cam2.mp4  dataset\\screen2.mp4     60.0        60.0   \n",
      "2   3  dataset\\cam3.mp4  dataset\\screen3.mp4     60.0        60.0   \n",
      "3   4  dataset\\cam4.mp4  dataset\\screen4.mp4     60.0        60.0   \n",
      "4   5  dataset\\cam5.mp4  dataset\\screen5.mp4     60.0        60.0   \n",
      "\n",
      "   duration_cam  duration_screen  \n",
      "0        211.08           211.32  \n",
      "1        216.58           218.45  \n",
      "2        271.48           271.25  \n",
      "3        194.70           209.95  \n",
      "4        147.05           148.15  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import cv2\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "VIDEO_FOLDER = \"dataset\" \n",
    "OUTPUT_CSV = \"video_inventory.csv\"\n",
    "# ---------------------\n",
    "\n",
    "def get_video_info(path):\n",
    "    \"\"\"Returns frame count, fps, and duration.\"\"\"\n",
    "    try:\n",
    "        cap = cv2.VideoCapture(path)\n",
    "        if not cap.isOpened():\n",
    "            return None, None, None\n",
    "        \n",
    "        frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        duration = frames / fps if fps > 0 else 0\n",
    "        \n",
    "        cap.release()\n",
    "        return frames, fps, duration\n",
    "    except:\n",
    "        return None, None, None\n",
    "\n",
    "def parse_id(filename):\n",
    "    match = re.search(r'(\\d+)', filename)\n",
    "    return int(match.group(1)) if match else None\n",
    "\n",
    "data_list = []\n",
    "print(\"Scanning dataset to build inventory...\")\n",
    "\n",
    "# Group by ID\n",
    "files = os.listdir(VIDEO_FOLDER)\n",
    "pairs = {}\n",
    "\n",
    "for f in files:\n",
    "    if not f.endswith(('.mp4', '.avi', '.mkv')):\n",
    "        continue\n",
    "    \n",
    "    vid_id = parse_id(f)\n",
    "    if vid_id is None:\n",
    "        continue\n",
    "        \n",
    "    full_path = os.path.join(VIDEO_FOLDER, f)\n",
    "    \n",
    "    if vid_id not in pairs:\n",
    "        pairs[vid_id] = {'cam': None, 'screen': None}\n",
    "        \n",
    "    if \"cam\" in f.lower():\n",
    "        pairs[vid_id]['cam'] = full_path\n",
    "    elif \"screen\" in f.lower():\n",
    "        pairs[vid_id]['screen'] = full_path\n",
    "\n",
    "# Build the Table\n",
    "for vid_id, pair in sorted(pairs.items()):\n",
    "    cam_path = pair['cam']\n",
    "    screen_path = pair['screen']\n",
    "    \n",
    "    # Only process complete pairs\n",
    "    if cam_path and screen_path:\n",
    "        # Get Technical Details (Check for corruption)\n",
    "        c_frames, c_fps, c_dur = get_video_info(cam_path)\n",
    "        s_frames, s_fps, s_dur = get_video_info(screen_path)\n",
    "        \n",
    "        if c_frames is None or s_frames is None:\n",
    "            print(f\"Warning: ID {vid_id} is corrupted/unreadable.\")\n",
    "            continue\n",
    "            \n",
    "        data_list.append({\n",
    "            'id': vid_id,\n",
    "            'cam_path': cam_path,\n",
    "            'screen_path': screen_path,\n",
    "            'cam_fps': round(c_fps, 2),\n",
    "            'screen_fps': round(s_fps, 2),\n",
    "            'duration_cam': round(c_dur, 2),\n",
    "            'duration_screen': round(s_dur, 2)\n",
    "        })\n",
    "\n",
    "# Save to CSV\n",
    "df = pd.DataFrame(data_list)\n",
    "df.to_csv(OUTPUT_CSV, index=False)\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(f\"Inventory saved to {OUTPUT_CSV}\")\n",
    "print(f\"Total Valid Pairs: {len(df)}\")\n",
    "print(\"-\" * 50)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d00c1e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting deepface\n",
      "  Downloading deepface-0.0.96-py3-none-any.whl (133 kB)\n",
      "     ------------------------------------ 133.1/133.1 kB 218.7 kB/s eta 0:00:00\n",
      "Collecting tf-keras\n",
      "  Downloading tf_keras-2.20.1-py3-none-any.whl (1.7 MB)\n",
      "     ---------------------------------------- 1.7/1.7 MB 883.6 kB/s eta 0:00:00\n",
      "Requirement already satisfied: requests>=2.27.1 in c:\\users\\janak\\onedrive\\desktop\\videoanalyzer\\.venv\\lib\\site-packages (from deepface) (2.32.5)\n",
      "Requirement already satisfied: numpy>=1.14.0 in c:\\users\\janak\\onedrive\\desktop\\videoanalyzer\\.venv\\lib\\site-packages (from deepface) (2.2.6)\n",
      "Requirement already satisfied: pandas>=0.23.4 in c:\\users\\janak\\onedrive\\desktop\\videoanalyzer\\.venv\\lib\\site-packages (from deepface) (2.3.3)\n",
      "Collecting gdown>=3.10.1\n",
      "  Downloading gdown-5.2.0-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: tqdm>=4.30.0 in c:\\users\\janak\\onedrive\\desktop\\videoanalyzer\\.venv\\lib\\site-packages (from deepface) (4.67.1)\n",
      "Requirement already satisfied: Pillow>=5.2.0 in c:\\users\\janak\\onedrive\\desktop\\videoanalyzer\\.venv\\lib\\site-packages (from deepface) (11.3.0)\n",
      "Requirement already satisfied: opencv-python>=4.5.5.64 in c:\\users\\janak\\onedrive\\desktop\\videoanalyzer\\.venv\\lib\\site-packages (from deepface) (4.12.0.88)\n",
      "Collecting tensorflow>=1.9.0\n",
      "  Downloading tensorflow-2.20.0-cp311-cp311-win_amd64.whl (331.8 MB)\n",
      "     -------------------------------------- 331.8/331.8 MB 1.4 MB/s eta 0:00:00\n",
      "Collecting keras>=2.2.0\n",
      "  Downloading keras-3.12.0-py3-none-any.whl (1.5 MB)\n",
      "     ---------------------------------------- 1.5/1.5 MB 2.6 MB/s eta 0:00:00\n",
      "Collecting Flask>=1.1.2\n",
      "  Downloading flask-3.1.2-py3-none-any.whl (103 kB)\n",
      "     -------------------------------------- 103.3/103.3 kB 2.0 MB/s eta 0:00:00\n",
      "Collecting flask-cors>=4.0.1\n",
      "  Downloading flask_cors-6.0.2-py3-none-any.whl (13 kB)\n",
      "Collecting mtcnn>=0.1.0\n",
      "  Downloading mtcnn-1.0.0-py3-none-any.whl (1.9 MB)\n",
      "     ---------------------------------------- 1.9/1.9 MB 3.5 MB/s eta 0:00:00\n",
      "Collecting retina-face>=0.0.14\n",
      "  Downloading retina_face-0.0.17-py3-none-any.whl (25 kB)\n",
      "Collecting fire>=0.4.0\n",
      "  Downloading fire-0.7.1-py3-none-any.whl (115 kB)\n",
      "     -------------------------------------- 115.9/115.9 kB 1.1 MB/s eta 0:00:00\n",
      "Collecting gunicorn>=20.1.0\n",
      "  Downloading gunicorn-23.0.0-py3-none-any.whl (85 kB)\n",
      "     ---------------------------------------- 85.0/85.0 kB 5.0 MB/s eta 0:00:00\n",
      "Collecting lightphe>=0.0.15\n",
      "  Downloading lightphe-0.0.19-py3-none-any.whl (59 kB)\n",
      "     ---------------------------------------- 59.4/59.4 kB 3.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: termcolor in c:\\users\\janak\\onedrive\\desktop\\videoanalyzer\\.venv\\lib\\site-packages (from fire>=0.4.0->deepface) (3.2.0)\n",
      "Collecting blinker>=1.9.0\n",
      "  Downloading blinker-1.9.0-py3-none-any.whl (8.5 kB)\n",
      "Collecting click>=8.1.3\n",
      "  Downloading click-8.3.1-py3-none-any.whl (108 kB)\n",
      "     -------------------------------------- 108.3/108.3 kB 3.2 MB/s eta 0:00:00\n",
      "Collecting itsdangerous>=2.2.0\n",
      "  Downloading itsdangerous-2.2.0-py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: jinja2>=3.1.2 in c:\\users\\janak\\onedrive\\desktop\\videoanalyzer\\.venv\\lib\\site-packages (from Flask>=1.1.2->deepface) (3.1.3)\n",
      "Requirement already satisfied: markupsafe>=2.1.1 in c:\\users\\janak\\onedrive\\desktop\\videoanalyzer\\.venv\\lib\\site-packages (from Flask>=1.1.2->deepface) (2.1.5)\n",
      "Collecting werkzeug>=3.1.0\n",
      "  Downloading werkzeug-3.1.4-py3-none-any.whl (224 kB)\n",
      "     -------------------------------------- 225.0/225.0 kB 4.7 MB/s eta 0:00:00\n",
      "Collecting beautifulsoup4\n",
      "  Downloading beautifulsoup4-4.14.3-py3-none-any.whl (107 kB)\n",
      "     -------------------------------------- 107.7/107.7 kB 1.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: filelock in c:\\users\\janak\\onedrive\\desktop\\videoanalyzer\\.venv\\lib\\site-packages (from gdown>=3.10.1->deepface) (3.19.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\janak\\onedrive\\desktop\\videoanalyzer\\.venv\\lib\\site-packages (from gunicorn>=20.1.0->deepface) (25.0)\n",
      "Collecting absl-py\n",
      "  Downloading absl_py-2.3.1-py3-none-any.whl (135 kB)\n",
      "     -------------------------------------- 135.8/135.8 kB 7.8 MB/s eta 0:00:00\n",
      "Collecting rich\n",
      "  Downloading rich-14.2.0-py3-none-any.whl (243 kB)\n",
      "     -------------------------------------- 243.4/243.4 kB 2.5 MB/s eta 0:00:00\n",
      "Collecting namex\n",
      "  Downloading namex-0.1.0-py3-none-any.whl (5.9 kB)\n",
      "Collecting h5py\n",
      "  Downloading h5py-3.15.1-cp311-cp311-win_amd64.whl (2.9 MB)\n",
      "     ---------------------------------------- 2.9/2.9 MB 2.9 MB/s eta 0:00:00\n",
      "Collecting optree\n",
      "  Downloading optree-0.18.0-cp311-cp311-win_amd64.whl (312 kB)\n",
      "     -------------------------------------- 312.2/312.2 kB 3.2 MB/s eta 0:00:00\n",
      "Collecting ml-dtypes\n",
      "  Downloading ml_dtypes-0.5.4-cp311-cp311-win_amd64.whl (210 kB)\n",
      "     -------------------------------------- 210.7/210.7 kB 2.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: sympy>=1.12 in c:\\users\\janak\\onedrive\\desktop\\videoanalyzer\\.venv\\lib\\site-packages (from lightphe>=0.0.15->deepface) (1.13.1)\n",
      "Collecting pytest>=7.1.2\n",
      "  Downloading pytest-9.0.2-py3-none-any.whl (374 kB)\n",
      "     -------------------------------------- 374.8/374.8 kB 3.9 MB/s eta 0:00:00\n",
      "Collecting lightecc\n",
      "  Downloading lightecc-0.0.3-py3-none-any.whl (44 kB)\n",
      "     -------------------------------------- 44.6/44.6 kB 555.0 kB/s eta 0:00:00\n",
      "Requirement already satisfied: joblib>=1.4.2 in c:\\users\\janak\\onedrive\\desktop\\videoanalyzer\\.venv\\lib\\site-packages (from mtcnn>=0.1.0->deepface) (1.5.2)\n",
      "Collecting lz4>=4.3.3\n",
      "  Downloading lz4-4.4.5-cp311-cp311-win_amd64.whl (99 kB)\n",
      "     ---------------------------------------- 99.5/99.5 kB 1.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\janak\\onedrive\\desktop\\videoanalyzer\\.venv\\lib\\site-packages (from pandas>=0.23.4->deepface) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\janak\\onedrive\\desktop\\videoanalyzer\\.venv\\lib\\site-packages (from pandas>=0.23.4->deepface) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\janak\\onedrive\\desktop\\videoanalyzer\\.venv\\lib\\site-packages (from pandas>=0.23.4->deepface) (2025.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\janak\\onedrive\\desktop\\videoanalyzer\\.venv\\lib\\site-packages (from requests>=2.27.1->deepface) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\janak\\onedrive\\desktop\\videoanalyzer\\.venv\\lib\\site-packages (from requests>=2.27.1->deepface) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\janak\\onedrive\\desktop\\videoanalyzer\\.venv\\lib\\site-packages (from requests>=2.27.1->deepface) (2.6.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\janak\\onedrive\\desktop\\videoanalyzer\\.venv\\lib\\site-packages (from requests>=2.27.1->deepface) (2025.11.12)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting flatbuffers>=24.3.25\n",
      "  Downloading flatbuffers-25.9.23-py2.py3-none-any.whl (30 kB)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1\n",
      "  Downloading gast-0.7.0-py3-none-any.whl (22 kB)\n",
      "Collecting google_pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "     ---------------------------------------- 57.5/57.5 kB 3.1 MB/s eta 0:00:00\n",
      "Collecting libclang>=13.0.0\n",
      "  Downloading libclang-18.1.1-py2.py3-none-win_amd64.whl (26.4 MB)\n",
      "     -------------------------------------- 26.4/26.4 MB 968.1 kB/s eta 0:00:00\n",
      "Collecting opt_einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "     -------------------------------------- 71.9/71.9 kB 995.0 kB/s eta 0:00:00\n",
      "Collecting protobuf>=5.28.0\n",
      "  Downloading protobuf-6.33.2-cp310-abi3-win_amd64.whl (436 kB)\n",
      "     ------------------------------------ 436.9/436.9 kB 738.2 kB/s eta 0:00:00\n",
      "Requirement already satisfied: setuptools in c:\\users\\janak\\onedrive\\desktop\\videoanalyzer\\.venv\\lib\\site-packages (from tensorflow>=1.9.0->deepface) (65.5.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\janak\\onedrive\\desktop\\videoanalyzer\\.venv\\lib\\site-packages (from tensorflow>=1.9.0->deepface) (1.17.0)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in c:\\users\\janak\\onedrive\\desktop\\videoanalyzer\\.venv\\lib\\site-packages (from tensorflow>=1.9.0->deepface) (4.15.0)\n",
      "Collecting wrapt>=1.11.0\n",
      "  Downloading wrapt-2.0.1-cp311-cp311-win_amd64.whl (60 kB)\n",
      "     ---------------------------------------- 60.4/60.4 kB 1.1 MB/s eta 0:00:00\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.76.0-cp311-cp311-win_amd64.whl (4.7 MB)\n",
      "     ---------------------------------------- 4.7/4.7 MB 1.2 MB/s eta 0:00:00\n",
      "Collecting tensorboard~=2.20.0\n",
      "  Downloading tensorboard-2.20.0-py3-none-any.whl (5.5 MB)\n",
      "     ---------------------------------------- 5.5/5.5 MB 1.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: colorama in c:\\users\\janak\\onedrive\\desktop\\videoanalyzer\\.venv\\lib\\site-packages (from tqdm>=4.30.0->deepface) (0.4.6)\n",
      "Collecting wheel<1.0,>=0.23.0\n",
      "  Downloading wheel-0.45.1-py3-none-any.whl (72 kB)\n",
      "     ---------------------------------------- 72.5/72.5 kB 1.3 MB/s eta 0:00:00\n",
      "Collecting iniconfig>=1.0.1\n",
      "  Downloading iniconfig-2.3.0-py3-none-any.whl (7.5 kB)\n",
      "Collecting pluggy<2,>=1.5\n",
      "  Downloading pluggy-1.6.0-py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: pygments>=2.7.2 in c:\\users\\janak\\onedrive\\desktop\\videoanalyzer\\.venv\\lib\\site-packages (from pytest>=7.1.2->lightphe>=0.0.15->deepface) (2.19.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\janak\\onedrive\\desktop\\videoanalyzer\\.venv\\lib\\site-packages (from sympy>=1.12->lightphe>=0.0.15->deepface) (1.3.0)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading markdown-3.10-py3-none-any.whl (107 kB)\n",
      "     -------------------------------------- 107.7/107.7 kB 3.1 MB/s eta 0:00:00\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Collecting soupsieve>=1.6.1\n",
      "  Downloading soupsieve-2.8-py3-none-any.whl (36 kB)\n",
      "Collecting pytest>=7.1.2\n",
      "  Downloading pytest-7.1.2-py3-none-any.whl (297 kB)\n",
      "     -------------------------------------- 297.0/297.0 kB 2.0 MB/s eta 0:00:00\n",
      "Collecting attrs>=19.2.0\n",
      "  Downloading attrs-25.4.0-py3-none-any.whl (67 kB)\n",
      "     ---------------------------------------- 67.6/67.6 kB 1.2 MB/s eta 0:00:00\n",
      "Collecting py>=1.8.2\n",
      "  Downloading py-1.11.0-py2.py3-none-any.whl (98 kB)\n",
      "     ---------------------------------------- 98.7/98.7 kB 1.4 MB/s eta 0:00:00\n",
      "Collecting tomli>=1.0.0\n",
      "  Downloading tomli-2.3.0-cp311-cp311-win_amd64.whl (107 kB)\n",
      "     -------------------------------------- 107.2/107.2 kB 2.1 MB/s eta 0:00:00\n",
      "Collecting atomicwrites>=1.0\n",
      "  Downloading atomicwrites-1.4.1.tar.gz (14 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting PySocks!=1.5.7,>=1.5.6\n",
      "  Downloading PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Collecting markdown-it-py>=2.2.0\n",
      "  Downloading markdown_it_py-4.0.0-py3-none-any.whl (87 kB)\n",
      "     ---------------------------------------- 87.3/87.3 kB 1.6 MB/s eta 0:00:00\n",
      "Collecting mdurl~=0.1\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: namex, libclang, flatbuffers, wrapt, wheel, werkzeug, tomli, tensorboard-data-server, soupsieve, PySocks, py, protobuf, pluggy, optree, opt_einsum, ml-dtypes, mdurl, markdown, lz4, itsdangerous, iniconfig, h5py, gunicorn, grpcio, google_pasta, gast, fire, click, blinker, attrs, atomicwrites, absl-py, tensorboard, pytest, mtcnn, markdown-it-py, Flask, beautifulsoup4, astunparse, rich, lightecc, gdown, flask-cors, lightphe, keras, tensorflow, tf-keras, retina-face, deepface\n",
      "  Running setup.py install for atomicwrites: started\n",
      "  Running setup.py install for atomicwrites: finished with status 'done'\n",
      "Successfully installed Flask-3.1.2 PySocks-1.7.1 absl-py-2.3.1 astunparse-1.6.3 atomicwrites-1.4.1 attrs-25.4.0 beautifulsoup4-4.14.3 blinker-1.9.0 click-8.3.1 deepface-0.0.96 fire-0.7.1 flask-cors-6.0.2 flatbuffers-25.9.23 gast-0.7.0 gdown-5.2.0 google_pasta-0.2.0 grpcio-1.76.0 gunicorn-23.0.0 h5py-3.15.1 iniconfig-2.3.0 itsdangerous-2.2.0 keras-3.12.0 libclang-18.1.1 lightecc-0.0.3 lightphe-0.0.19 lz4-4.4.5 markdown-3.10 markdown-it-py-4.0.0 mdurl-0.1.2 ml-dtypes-0.5.4 mtcnn-1.0.0 namex-0.1.0 opt_einsum-3.4.0 optree-0.18.0 pluggy-1.6.0 protobuf-6.33.2 py-1.11.0 pytest-7.1.2 retina-face-0.0.17 rich-14.2.0 soupsieve-2.8 tensorboard-2.20.0 tensorboard-data-server-0.7.2 tensorflow-2.20.0 tf-keras-2.20.1 tomli-2.3.0 werkzeug-3.1.4 wheel-0.45.1 wrapt-2.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  DEPRECATION: atomicwrites is being installed using the legacy 'setup.py install' method, because it does not have a 'pyproject.toml' and the 'wheel' package is not installed. pip 23.1 will enforce this behaviour change. A possible replacement is to enable the '--use-pep517' option. Discussion can be found at https://github.com/pypa/pip/issues/8559\n",
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    }
   ],
   "source": [
    "%pip install deepface tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e55ee053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\janak\\OneDrive\\Desktop\\VideoAnalyzer\\.venv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "25-12-14 03:13:45 - Directory C:\\Users\\janak\\.deepface has been created\n",
      "25-12-14 03:13:45 - Directory C:\\Users\\janak\\.deepface\\weights has been created\n",
      "Loading Inventory...\n",
      "Starting Auto-Labeling for 102 videos...\n",
      "This uses the 'cam' video to detect user frustration.\n",
      "Processing ID 1...25-12-14 03:13:46 -  facial_expression_model_weights.h5 will be downloaded from https://github.com/serengil/deepface_models/releases/download/v1.0/facial_expression_model_weights.h5 to C:\\Users\\janak\\.deepface\\weights\\facial_expression_model_weights.h5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://github.com/serengil/deepface_models/releases/download/v1.0/facial_expression_model_weights.h5\n",
      "To: C:\\Users\\janak\\.deepface\\weights\\facial_expression_model_weights.h5\n",
      "100%|| 5.98M/5.98M [00:09<00:00, 651kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> CLEAN (Frustration: 9.0%)\n",
      "Processing ID 2... -> FLAGGED (Frustration: 72.7%)\n",
      "Processing ID 3... -> FLAGGED (Frustration: 24.5%)\n",
      "Processing ID 4... -> FLAGGED (Frustration: 33.2%)\n",
      "Processing ID 5... -> FLAGGED (Frustration: 91.5%)\n",
      "Processing ID 6... -> FLAGGED (Frustration: 24.9%)\n",
      "Processing ID 7... -> FLAGGED (Frustration: 42.7%)\n",
      "Processing ID 8... -> CLEAN (Frustration: 0.6%)\n",
      "Processing ID 9... -> FLAGGED (Frustration: 20.7%)\n",
      "Processing ID 10... -> CLEAN (Frustration: 5.8%)\n",
      "Processing ID 11... -> CLEAN (Frustration: 3.6%)\n",
      "Processing ID 12... -> FLAGGED (Frustration: 31.3%)\n",
      "Processing ID 13... -> CLEAN (Frustration: 5.3%)\n",
      "Processing ID 14... -> FLAGGED (Frustration: 20.1%)\n",
      "Processing ID 15... -> CLEAN (Frustration: 4.6%)\n",
      "Processing ID 16... -> FLAGGED (Frustration: 12.5%)\n",
      "Processing ID 17... -> CLEAN (Frustration: 2.8%)\n",
      "Processing ID 18... -> CLEAN (Frustration: 5.9%)\n",
      "Processing ID 19... -> FLAGGED (Frustration: 12.2%)\n",
      "Processing ID 20... -> FLAGGED (Frustration: 56.3%)\n",
      "Processing ID 21... -> FLAGGED (Frustration: 52.7%)\n",
      "Processing ID 22... -> CLEAN (Frustration: 5.2%)\n",
      "Processing ID 23... -> FLAGGED (Frustration: 10.7%)\n",
      "Processing ID 24... -> CLEAN (Frustration: 4.8%)\n",
      "Processing ID 25... -> CLEAN (Frustration: 8.0%)\n",
      "Processing ID 26... -> CLEAN (Frustration: 7.3%)\n",
      "Processing ID 27... -> CLEAN (Frustration: 4.8%)\n",
      "Processing ID 28... -> FLAGGED (Frustration: 12.5%)\n",
      "Processing ID 29... -> FLAGGED (Frustration: 12.2%)\n",
      "Processing ID 30... -> FLAGGED (Frustration: 20.8%)\n",
      "Processing ID 31... -> FLAGGED (Frustration: 73.5%)\n",
      "Processing ID 32... -> CLEAN (Frustration: 8.6%)\n",
      "Processing ID 33... -> FLAGGED (Frustration: 16.4%)\n",
      "Processing ID 34... -> FLAGGED (Frustration: 80.6%)\n",
      "Processing ID 35... -> FLAGGED (Frustration: 56.1%)\n",
      "Processing ID 36... -> FLAGGED (Frustration: 57.2%)\n",
      "Processing ID 37... -> CLEAN (Frustration: 1.7%)\n",
      "Processing ID 38... -> FLAGGED (Frustration: 72.9%)\n",
      "Processing ID 39... -> FLAGGED (Frustration: 93.7%)\n",
      "Processing ID 40... -> CLEAN (Frustration: 2.3%)\n",
      "Processing ID 41... -> CLEAN (Frustration: 2.0%)\n",
      "Processing ID 42... -> CLEAN (Frustration: 8.2%)\n",
      "Processing ID 43... -> FLAGGED (Frustration: 12.6%)\n",
      "Processing ID 44... -> CLEAN (Frustration: 7.9%)\n",
      "Processing ID 45... -> FLAGGED (Frustration: 10.5%)\n",
      "Processing ID 46... -> FLAGGED (Frustration: 17.3%)\n",
      "Processing ID 47... -> FLAGGED (Frustration: 33.7%)\n",
      "Processing ID 48... -> CLEAN (Frustration: 8.8%)\n",
      "Processing ID 49... -> FLAGGED (Frustration: 45.6%)\n",
      "Processing ID 50... -> CLEAN (Frustration: 1.5%)\n",
      "Processing ID 51... -> CLEAN (Frustration: 0.4%)\n",
      "Processing ID 52... -> CLEAN (Frustration: 0.6%)\n",
      "Processing ID 53... -> CLEAN (Frustration: 0.0%)\n",
      "Processing ID 54... -> FLAGGED (Frustration: 27.2%)\n",
      "Processing ID 55... -> FLAGGED (Frustration: 27.7%)\n",
      "Processing ID 57... -> FLAGGED (Frustration: 38.4%)\n",
      "Processing ID 58... -> CLEAN (Frustration: 1.9%)\n",
      "Processing ID 59... -> FLAGGED (Frustration: 14.1%)\n",
      "Processing ID 60... -> FLAGGED (Frustration: 18.8%)\n",
      "Processing ID 61... -> FLAGGED (Frustration: 26.9%)\n",
      "Processing ID 62... -> FLAGGED (Frustration: 48.0%)\n",
      "Processing ID 63... -> FLAGGED (Frustration: 76.7%)\n",
      "Processing ID 64... -> FLAGGED (Frustration: 49.1%)\n",
      "Processing ID 65... -> FLAGGED (Frustration: 52.9%)\n",
      "Processing ID 66... -> CLEAN (Frustration: 7.4%)\n",
      "Processing ID 67... -> FLAGGED (Frustration: 55.5%)\n",
      "Processing ID 68... -> FLAGGED (Frustration: 69.2%)\n",
      "Processing ID 69... -> FLAGGED (Frustration: 48.5%)\n",
      "Processing ID 70... -> FLAGGED (Frustration: 79.6%)\n",
      "Processing ID 71... -> FLAGGED (Frustration: 89.5%)\n",
      "Processing ID 72... -> FLAGGED (Frustration: 17.8%)\n",
      "Processing ID 73... -> FLAGGED (Frustration: 67.7%)\n",
      "Processing ID 74... -> CLEAN (Frustration: 1.6%)\n",
      "Processing ID 75... -> CLEAN (Frustration: 5.6%)\n",
      "Processing ID 76... -> FLAGGED (Frustration: 14.1%)\n",
      "Processing ID 77... -> CLEAN (Frustration: 2.9%)\n",
      "Processing ID 78... -> FLAGGED (Frustration: 22.3%)\n",
      "Processing ID 79... -> FLAGGED (Frustration: 25.0%)\n",
      "Processing ID 80... -> CLEAN (Frustration: 9.2%)\n",
      "Processing ID 81... -> CLEAN (Frustration: 5.7%)\n",
      "Processing ID 82... -> FLAGGED (Frustration: 27.5%)\n",
      "Processing ID 83... -> FLAGGED (Frustration: 20.6%)\n",
      "Processing ID 84... -> CLEAN (Frustration: 3.2%)\n",
      "Processing ID 85... -> FLAGGED (Frustration: 10.2%)\n",
      "Processing ID 86... -> CLEAN (Frustration: 8.9%)\n",
      "Processing ID 87... -> CLEAN (Frustration: 4.7%)\n",
      "Processing ID 88... -> CLEAN (Frustration: 9.5%)\n",
      "Processing ID 89... -> FLAGGED (Frustration: 42.5%)\n",
      "Processing ID 90... -> FLAGGED (Frustration: 74.1%)\n",
      "Processing ID 91... -> FLAGGED (Frustration: 19.5%)\n",
      "Processing ID 92... -> FLAGGED (Frustration: 92.5%)\n",
      "Processing ID 93... -> FLAGGED (Frustration: 24.2%)\n",
      "Processing ID 94... -> CLEAN (Frustration: 9.7%)\n",
      "Processing ID 95... -> CLEAN (Frustration: 10.0%)\n",
      "Processing ID 96... -> FLAGGED (Frustration: 20.2%)\n",
      "Processing ID 97... -> CLEAN (Frustration: 5.9%)\n",
      "Processing ID 98... -> CLEAN (Frustration: 4.7%)\n",
      "Processing ID 99... -> CLEAN (Frustration: 4.9%)\n",
      "Processing ID 100... -> FLAGGED (Frustration: 15.3%)\n",
      "Processing ID 101... -> FLAGGED (Frustration: 18.6%)\n",
      "Processing ID 102... -> FLAGGED (Frustration: 24.1%)\n",
      "Processing ID 103... -> CLEAN (Frustration: 9.2%)\n",
      "--------------------------------------------------\n",
      "Auto-Labeling Complete!\n",
      "Saved to labeled_dataset.csv\n",
      "Breakdown:\n",
      "label\n",
      "1    61\n",
      "0    41\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import cv2\n",
    "from deepface import DeepFace\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "CSV_FILE = \"video_inventory.csv\"\n",
    "OUTPUT_CSV = \"labeled_dataset.csv\"\n",
    "SAMPLE_RATE = 30  # Analyze 1 frame every 30 frames (approx 1 per sec at 30fps)\n",
    "# ---------------------\n",
    "\n",
    "def analyze_emotions(video_path):\n",
    "    \"\"\"Scans a video and returns the % of frames showing negative emotion.\"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    total_frames = 0\n",
    "    negative_frames = 0\n",
    "    \n",
    "    # We look for these \"Frustration\" indicators\n",
    "    negative_emotions = ['angry', 'disgust', 'fear', 'sad']\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "            \n",
    "        total_frames += 1\n",
    "        \n",
    "        # Skip frames to speed up (process 1 frame every second roughly)\n",
    "        if total_frames % SAMPLE_RATE != 0:\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            # DeepFace expects RGB\n",
    "            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Analyze face (enforce_detection=False skips frames with no face)\n",
    "            result = DeepFace.analyze(rgb_frame, actions=['emotion'], \n",
    "                                    enforce_detection=False, silent=True)\n",
    "            \n",
    "            # Result is a list of dicts (one for each face found)\n",
    "            if result:\n",
    "                # Get dominant emotion\n",
    "                emotion = result[0]['dominant_emotion']\n",
    "                if emotion in negative_emotions:\n",
    "                    negative_frames += 1\n",
    "                    \n",
    "        except Exception as e:\n",
    "            pass # Skip frames where face detection fails\n",
    "            \n",
    "    cap.release()\n",
    "    \n",
    "    # Calculate Frustration Score\n",
    "    scanned_frames = total_frames // SAMPLE_RATE\n",
    "    if scanned_frames == 0: return 0.0\n",
    "    \n",
    "    frustration_score = negative_frames / scanned_frames\n",
    "    return frustration_score\n",
    "\n",
    "# --- MAIN LOOP ---\n",
    "print(\"Loading Inventory...\")\n",
    "df = pd.read_csv(CSV_FILE)\n",
    "df['frustration_score'] = 0.0\n",
    "df['label'] = 0  # Default to PASS\n",
    "\n",
    "print(f\"Starting Auto-Labeling for {len(df)} videos...\")\n",
    "print(\"This uses the 'cam' video to detect user frustration.\")\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    cam_path = row['cam_path']\n",
    "    vid_id = row['id']\n",
    "    \n",
    "    if os.path.exists(cam_path):\n",
    "        print(f\"Processing ID {vid_id}...\", end=\"\")\n",
    "        \n",
    "        score = analyze_emotions(cam_path)\n",
    "        df.at[index, 'frustration_score'] = round(score, 3)\n",
    "        \n",
    "        # HEURISTIC RULE:\n",
    "        # If user is frustrated in more than 10% of the video -> Label as FAIL (1)\n",
    "        if score > 0.10:\n",
    "            df.at[index, 'label'] = 1\n",
    "            print(f\" -> FLAGGED (Frustration: {score:.1%})\")\n",
    "        else:\n",
    "            print(f\" -> CLEAN (Frustration: {score:.1%})\")\n",
    "            \n",
    "    # Save periodically\n",
    "    if index % 5 == 0:\n",
    "        df.to_csv(OUTPUT_CSV, index=False)\n",
    "\n",
    "# Final Save\n",
    "df.to_csv(OUTPUT_CSV, index=False)\n",
    "print(\"-\" * 50)\n",
    "print(\"Auto-Labeling Complete!\")\n",
    "print(f\"Saved to {OUTPUT_CSV}\")\n",
    "print(\"Breakdown:\")\n",
    "print(df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b86a5d5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Videos:   81\n",
      "Validation Videos: 21\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the labeled data\n",
    "df = pd.read_csv(\"labeled_dataset.csv\")\n",
    "\n",
    "# Split 80% Train, 20% Validation\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, stratify=df['label'], random_state=42)\n",
    "\n",
    "# Save separate CSVs\n",
    "train_df.to_csv(\"train.csv\", index=False)\n",
    "val_df.to_csv(\"val.csv\", index=False)\n",
    "\n",
    "print(f\"Training Videos:   {len(train_df)}\")\n",
    "print(f\"Validation Videos: {len(val_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79d9efd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Class Defined Successfully (with Hotfix).\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torchvision\n",
    "\n",
    "# --- HOTFIX FOR PYTORCHVIDEO ---\n",
    "# PytorchVideo looks for 'functional_tensor' which was removed in newer TorchVision.\n",
    "# We map it to the new location 'functional'.\n",
    "sys.modules['torchvision.transforms.functional_tensor'] = torchvision.transforms.functional\n",
    "# -------------------------------\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd # Make sure pandas is imported\n",
    "\n",
    "# Now we can safely import pytorchvideo\n",
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey,\n",
    "    ShortSideScale,\n",
    "    UniformTemporalSubsample\n",
    ")\n",
    "from torchvision.transforms import Compose, Lambda\n",
    "from torchvision.transforms._transforms_video import CenterCropVideo, NormalizeVideo\n",
    "\n",
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform=None, num_frames=32):\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "        self.num_frames = num_frames\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # 1. Get Path and Label\n",
    "        row = self.df.iloc[idx]\n",
    "        video_path = row['screen_path'] # We train on SCREEN first\n",
    "        label = int(row['label'])\n",
    "        \n",
    "        # 2. Load Video\n",
    "        frames = self._load_video(video_path)\n",
    "        \n",
    "        # 3. Apply Transforms (Resize, Normalize)\n",
    "        if self.transform:\n",
    "            frames = self.transform(frames)\n",
    "            \n",
    "        return frames, label\n",
    "\n",
    "    def _load_video(self, path):\n",
    "        \"\"\"Reads video frames using OpenCV\"\"\"\n",
    "        cap = cv2.VideoCapture(path)\n",
    "        frames = []\n",
    "        try:\n",
    "            while True:\n",
    "                ret, frame = cap.read()\n",
    "                if not ret: break\n",
    "                # Convert BGR to RGB\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                frames.append(frame)\n",
    "        finally:\n",
    "            cap.release()\n",
    "            \n",
    "        if len(frames) == 0:\n",
    "            # Return blank tensor if video fails\n",
    "            return torch.zeros((3, self.num_frames, 256, 256))\n",
    "\n",
    "        # Convert to Tensor (C, T, H, W)\n",
    "        video_tensor = torch.from_numpy(np.array(frames))\n",
    "        video_tensor = video_tensor.permute(3, 0, 1, 2) \n",
    "        \n",
    "        return video_tensor\n",
    "\n",
    "print(\"Dataset Class Defined Successfully (with Hotfix).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11dbd965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Loader...\n",
      "Label: 0\n",
      "Slow Pathway Shape: torch.Size([3, 8, 256, 256]) (Channels, Frames, Height, Width)\n",
      "Fast Pathway Shape: torch.Size([3, 32, 256, 256])\n",
      "SUCCESS: Data shapes are correct for SlowFast!\n"
     ]
    }
   ],
   "source": [
    "from pytorchvideo.data.encoded_video import EncodedVideo\n",
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey,\n",
    "    ShortSideScale,\n",
    "    UniformTemporalSubsample,\n",
    "    UniformCropVideo\n",
    ") \n",
    "from torchvision.transforms import Compose, Lambda\n",
    "from torchvision.transforms._transforms_video import NormalizeVideo\n",
    "\n",
    "# Define the Transforms (Resize -> Crop -> Normalize)\n",
    "side_size = 256\n",
    "mean = [0.45, 0.45, 0.45]\n",
    "std = [0.225, 0.225, 0.225]\n",
    "crop_size = 256\n",
    "num_frames = 32 # Model sees 32 frames at a time\n",
    "\n",
    "transform = ApplyTransformToKey(\n",
    "    key=\"video\",\n",
    "    transform=Compose(\n",
    "        [\n",
    "            UniformTemporalSubsample(num_frames),\n",
    "            Lambda(lambda x: x / 255.0),\n",
    "            NormalizeVideo(mean, std),\n",
    "            ShortSideScale(size=side_size),\n",
    "            CenterCropVideo(crop_size),\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Pack Pathway Logic for SlowFast\n",
    "# Alpha=4 means Fast path has 4x more frames (or sample rate diff)\n",
    "def pack_pathway_output(frames):\n",
    "    fast_pathway = frames\n",
    "    # Slow pathway: subsample by factor of 4\n",
    "    slow_pathway = torch.index_select(\n",
    "        frames,\n",
    "        1,\n",
    "        torch.linspace(0, frames.shape[1] - 1, frames.shape[1] // 4).long(),\n",
    "    )\n",
    "    frame_list = [slow_pathway, fast_pathway]\n",
    "    return frame_list\n",
    "\n",
    "# Instantiate\n",
    "dataset = VideoDataset(csv_file=\"train.csv\", num_frames=num_frames)\n",
    "\n",
    "print(\"Testing Loader...\")\n",
    "# Manually grab one item\n",
    "raw_video, label = dataset[0] # Grab first video\n",
    "\n",
    "# Apply Transform manually to check\n",
    "video_data = {\"video\": raw_video}\n",
    "video_data = transform(video_data)\n",
    "final_input = pack_pathway_output(video_data[\"video\"])\n",
    "\n",
    "print(f\"Label: {label}\")\n",
    "print(f\"Slow Pathway Shape: {final_input[0].shape} (Channels, Frames, Height, Width)\")\n",
    "print(f\"Fast Pathway Shape: {final_input[1].shape}\")\n",
    "\n",
    "if final_input[0].shape[0] == 3 and final_input[0].shape[2] == 256:\n",
    "    print(\"SUCCESS: Data shapes are correct for SlowFast!\")\n",
    "else:\n",
    "    print(\"ERROR: Shapes are wrong.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e7b51c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading/Loading SlowFast Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\janak/.cache\\torch\\hub\\facebookresearch_pytorchvideo_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded and moved to cuda\n",
      "New Output Layer: Linear(in_features=2304, out_features=2, bias=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def get_model(num_classes=2):\n",
    "    # 1. Load pre-trained SlowFast from PyTorch Hub\n",
    "    # This downloads the weights (approx 300MB) the first time you run it.\n",
    "    model = torch.hub.load('facebookresearch/pytorchvideo', 'slowfast_r50', pretrained=True)\n",
    "    \n",
    "    # 2. Freeze the early layers (Optional but recommended for small datasets)\n",
    "    # This prevents the model from forgetting \"basic vision\" while learning your UI.\n",
    "    # for param in model.parameters():\n",
    "    #     param.requires_grad = False\n",
    "        \n",
    "    # 3. Perform Surgery on the Head\n",
    "    # The original model ends with a projection layer for 400 classes.\n",
    "    # We find that layer and replace it.\n",
    "    # In SlowFast, the head is typically at model.blocks[-1].proj\n",
    "    \n",
    "    input_features = model.blocks[-1].proj.in_features\n",
    "    model.blocks[-1].proj = nn.Linear(input_features, num_classes)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Initialize\n",
    "print(\"Downloading/Loading SlowFast Model...\")\n",
    "model = get_model(num_classes=2)\n",
    "\n",
    "# Move to GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"Model loaded and moved to {device}\")\n",
    "print(\"New Output Layer:\", model.blocks[-1].proj) \n",
    "# Should see: Linear(in_features=2304, out_features=2, bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73c6e53a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Class Updated (Dictionary Fix Applied).\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torchvision\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- HOTFIX RE-APPLY ---\n",
    "sys.modules['torchvision.transforms.functional_tensor'] = torchvision.transforms.functional\n",
    "# -----------------------\n",
    "\n",
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform=None, num_frames=32):\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "        self.num_frames = num_frames\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        video_path = row['screen_path']\n",
    "        label = int(row['label'])\n",
    "        \n",
    "        # 1. Load and Resize\n",
    "        frames = self._load_video(video_path)\n",
    "        \n",
    "        # 2. Apply Transforms\n",
    "        # CRITICAL FIX: Wrap in dictionary for pytorchvideo\n",
    "        if self.transform:\n",
    "            video_data = {\"video\": frames} \n",
    "            video_data = self.transform(video_data)\n",
    "            frames = video_data[\"video\"] # Extract back out\n",
    "            \n",
    "        return frames, label\n",
    "\n",
    "    def _load_video(self, path):\n",
    "        \"\"\"Reads video frames, resizing them instantly to save RAM.\"\"\"\n",
    "        cap = cv2.VideoCapture(path)\n",
    "        frames = []\n",
    "        try:\n",
    "            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "            # Skip frames to save time/memory\n",
    "            skip_step = max(1, total_frames // 64) \n",
    "            \n",
    "            count = 0\n",
    "            while True:\n",
    "                ret, frame = cap.read()\n",
    "                if not ret: break\n",
    "                \n",
    "                if count % skip_step == 0:\n",
    "                    # Resize to 256x256\n",
    "                    frame = cv2.resize(frame, (256, 256))\n",
    "                    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                    frames.append(frame)\n",
    "                \n",
    "                count += 1\n",
    "        finally:\n",
    "            cap.release()\n",
    "            \n",
    "        if len(frames) == 0:\n",
    "            return torch.zeros((3, self.num_frames, 256, 256))\n",
    "\n",
    "        # Convert to Tensor (C, T, H, W)\n",
    "        video_tensor = torch.from_numpy(np.array(frames))\n",
    "        video_tensor = video_tensor.permute(3, 0, 1, 2) \n",
    "        \n",
    "        return video_tensor\n",
    "\n",
    "print(\"Dataset Class Updated (Dictionary Fix Applied).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55582667",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\janak/.cache\\torch\\hub\\facebookresearch_pytorchvideo_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on cuda\n",
      "Starting Training for 5 epochs...\n",
      "\n",
      "--- Epoch 1/5 ---\n",
      "Batch 0: Loss = 0.7511\n",
      "Batch 5: Loss = 0.6088\n",
      "Batch 10: Loss = 0.4846\n",
      "Batch 15: Loss = 0.7605\n",
      "Batch 20: Loss = 1.1983\n",
      "Train Accuracy: 58.02% | Avg Loss: 0.7174\n",
      "Validation Accuracy: 61.90%\n",
      "\n",
      "--- Epoch 2/5 ---\n",
      "Batch 0: Loss = 0.8042\n",
      "Batch 5: Loss = 1.0805\n",
      "Batch 10: Loss = 0.6124\n",
      "Batch 15: Loss = 0.4326\n",
      "Batch 20: Loss = 0.2408\n",
      "Train Accuracy: 55.56% | Avg Loss: 0.6623\n",
      "Validation Accuracy: 61.90%\n",
      "\n",
      "--- Epoch 3/5 ---\n",
      "Batch 0: Loss = 0.5567\n",
      "Batch 5: Loss = 0.7762\n",
      "Batch 10: Loss = 0.4542\n",
      "Batch 15: Loss = 0.4847\n",
      "Batch 20: Loss = 0.8718\n",
      "Train Accuracy: 71.60% | Avg Loss: 0.5597\n",
      "Validation Accuracy: 42.86%\n",
      "\n",
      "--- Epoch 4/5 ---\n",
      "Batch 0: Loss = 0.4072\n",
      "Batch 5: Loss = 0.2513\n",
      "Batch 10: Loss = 0.2991\n",
      "Batch 15: Loss = 0.3583\n",
      "Batch 20: Loss = 0.4060\n",
      "Train Accuracy: 90.12% | Avg Loss: 0.4033\n",
      "Validation Accuracy: 42.86%\n",
      "\n",
      "--- Epoch 5/5 ---\n",
      "Batch 0: Loss = 0.3667\n",
      "Batch 5: Loss = 0.1686\n",
      "Batch 10: Loss = 0.3250\n",
      "Batch 15: Loss = 0.3279\n",
      "Batch 20: Loss = 0.7842\n",
      "Train Accuracy: 93.83% | Avg Loss: 0.3207\n",
      "Validation Accuracy: 47.62%\n",
      "\n",
      "TRAINING COMPLETE.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import Compose, Lambda\n",
    "from torchvision.transforms._transforms_video import CenterCropVideo, NormalizeVideo\n",
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey,\n",
    "    ShortSideScale,\n",
    "    UniformTemporalSubsample\n",
    ")\n",
    "\n",
    "# --- RE-LOAD MODEL ---\n",
    "# (We reload to be safe, though not strictly necessary if previous cell ran)\n",
    "def get_model(num_classes=2):\n",
    "    model = torch.hub.load('facebookresearch/pytorchvideo', 'slowfast_r50', pretrained=True)\n",
    "    input_features = model.blocks[-1].proj.in_features\n",
    "    model.blocks[-1].proj = nn.Linear(input_features, num_classes)\n",
    "    return model\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = get_model(num_classes=2)\n",
    "model = model.to(device)\n",
    "print(f\"Model loaded on {device}\")\n",
    "\n",
    "# --- HYPERPARAMETERS ---\n",
    "BATCH_SIZE = 4\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "# --- TRANSFORMS ---\n",
    "transform = ApplyTransformToKey(\n",
    "    key=\"video\",\n",
    "    transform=Compose([\n",
    "        UniformTemporalSubsample(32),\n",
    "        Lambda(lambda x: x / 255.0),\n",
    "        NormalizeVideo([0.45, 0.45, 0.45], [0.225, 0.225, 0.225]),\n",
    "        ShortSideScale(size=256),\n",
    "        CenterCropVideo(256),\n",
    "    ]),\n",
    ")\n",
    "\n",
    "# --- DATA LOADERS ---\n",
    "# (Assumes VideoDataset is defined from previous steps)\n",
    "train_dataset = VideoDataset(csv_file=\"train.csv\", transform=transform)\n",
    "val_dataset = VideoDataset(csv_file=\"val.csv\", transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "# --- OPTIMIZER ---\n",
    "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=0.9)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def pack_pathway(frames):\n",
    "    \"\"\"Creates Slow and Fast pathways from the video tensor.\"\"\"\n",
    "    fast = frames\n",
    "    # Subsample for Slow pathway (every 4th frame)\n",
    "    indices = torch.linspace(0, frames.shape[2] - 1, frames.shape[2] // 4).long()\n",
    "    \n",
    "    # CRITICAL FIX: Move indices to the same device as the video (GPU)\n",
    "    indices = indices.to(frames.device)\n",
    "    \n",
    "    slow = torch.index_select(frames, 2, indices)\n",
    "    return [slow, fast]\n",
    "\n",
    "# --- TRAINING LOOP ---\n",
    "print(f\"Starting Training for {NUM_EPOCHS} epochs...\")\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    print(f\"\\n--- Epoch {epoch+1}/{NUM_EPOCHS} ---\")\n",
    "    \n",
    "    for i, (video_batch, labels) in enumerate(train_loader):\n",
    "        # Move inputs to GPU\n",
    "        video_batch = video_batch.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Prepare Inputs (Now safe because indices move to GPU too)\n",
    "        inputs = pack_pathway(video_batch)\n",
    "        \n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward Pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward Pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Stats\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        if i % 5 == 0:\n",
    "            print(f\"Batch {i}: Loss = {loss.item():.4f}\")\n",
    "\n",
    "    train_acc = 100 * correct / total\n",
    "    print(f\"Train Accuracy: {train_acc:.2f}% | Avg Loss: {running_loss/len(train_loader):.4f}\")\n",
    "\n",
    "    # Validation Phase\n",
    "    model.eval()\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for video_batch, labels in val_loader:\n",
    "            video_batch = video_batch.to(device)\n",
    "            labels = labels.to(device)\n",
    "            inputs = pack_pathway(video_batch)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "            \n",
    "    val_acc = 100 * val_correct / val_total\n",
    "    print(f\"Validation Accuracy: {val_acc:.2f}%\")\n",
    "    \n",
    "    # Save the model state\n",
    "    torch.save(model.state_dict(), \"last_model.pth\")\n",
    "\n",
    "print(\"\\nTRAINING COMPLETE.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4505c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model on cuda...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\janak/.cache\\torch\\hub\\facebookresearch_pytorchvideo_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Testing Video: dataset\\screen47.mkv\n",
      " Actual Label:  FAIL (Bug)\n",
      "------------------------------\n",
      " AI Prediction: FAIL (Bug Detected)\n",
      " Confidence:    92.0%\n",
      " CORRECT!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "from pytorchvideo.transforms import ApplyTransformToKey, ShortSideScale, UniformTemporalSubsample\n",
    "from torchvision.transforms import Compose, Lambda\n",
    "from torchvision.transforms._transforms_video import CenterCropVideo, NormalizeVideo\n",
    "\n",
    "# --- RE-DEFINE ARCHITECTURE (Must match training) ---\n",
    "def get_model(num_classes=2):\n",
    "    model = torch.hub.load('facebookresearch/pytorchvideo', 'slowfast_r50', pretrained=True)\n",
    "    input_features = model.blocks[-1].proj.in_features\n",
    "    model.blocks[-1].proj = torch.nn.Linear(input_features, num_classes)\n",
    "    return model\n",
    "\n",
    "def pack_pathway(frames):\n",
    "    \"\"\"Creates Slow and Fast pathways.\"\"\"\n",
    "    fast = frames\n",
    "    indices = torch.linspace(0, frames.shape[2] - 1, frames.shape[2] // 4).long().to(frames.device)\n",
    "    slow = torch.index_select(frames, 2, indices)\n",
    "    return [slow, fast]\n",
    "\n",
    "# --- LOAD SAVED MODEL ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Loading model on {device}...\")\n",
    "\n",
    "model = get_model(num_classes=2)\n",
    "# Load the weights we just trained\n",
    "model.load_state_dict(torch.load(\"last_model.pth\"))\n",
    "model = model.to(device)\n",
    "model.eval() # Switch to evaluation mode\n",
    "\n",
    "# --- DEFINE TRANSFORM ---\n",
    "transform = ApplyTransformToKey(\n",
    "    key=\"video\",\n",
    "    transform=Compose([\n",
    "        UniformTemporalSubsample(32),\n",
    "        Lambda(lambda x: x / 255.0),\n",
    "        NormalizeVideo([0.45, 0.45, 0.45], [0.225, 0.225, 0.225]),\n",
    "        ShortSideScale(size=256),\n",
    "        CenterCropVideo(256),\n",
    "    ]),\n",
    ")\n",
    "\n",
    "# --- PICK A TEST VIDEO ---\n",
    "# We pick a random one from your list\n",
    "df = pd.read_csv(\"val.csv\")\n",
    "random_row = df.sample(1).iloc[0]\n",
    "video_path = random_row['screen_path']\n",
    "actual_label = int(random_row['label'])\n",
    "\n",
    "print(f\"\\n Testing Video: {video_path}\")\n",
    "print(f\" Actual Label:  {'FAIL (Bug)' if actual_label == 1 else 'PASS (Clean)'}\")\n",
    "\n",
    "# --- PROCESS & PREDICT ---\n",
    "def predict(path):\n",
    "    # 1. Read Video\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    frames = []\n",
    "    try:\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        skip_step = max(1, total_frames // 64) \n",
    "        count = 0\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret: break\n",
    "            if count % skip_step == 0:\n",
    "                frame = cv2.resize(frame, (256, 256))\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                frames.append(frame)\n",
    "            count += 1\n",
    "    finally:\n",
    "        cap.release()\n",
    "\n",
    "    if len(frames) == 0: return \"Error\", 0.0\n",
    "\n",
    "    # 2. Prepare Tensor\n",
    "    video_tensor = torch.from_numpy(np.array(frames)).permute(3, 0, 1, 2)\n",
    "    video_data = {\"video\": video_tensor}\n",
    "    video_data = transform(video_data)\n",
    "    \n",
    "    # 3. Pack Pathway & Move to GPU\n",
    "    inputs = pack_pathway(video_data[\"video\"].unsqueeze(0)) # Add batch dim\n",
    "    inputs = [i.to(device) for i in inputs]\n",
    "    \n",
    "    # 4. Predict\n",
    "    with torch.no_grad():\n",
    "        outputs = model(inputs)\n",
    "        probs = torch.nn.functional.softmax(outputs, dim=1)\n",
    "        # Class 0 = PASS, Class 1 = FAIL\n",
    "        pred_idx = torch.argmax(probs, dim=1).item()\n",
    "        confidence = probs[0][pred_idx].item()\n",
    "        \n",
    "    return pred_idx, confidence\n",
    "\n",
    "pred, conf = predict(video_path)\n",
    "\n",
    "print(\"-\" * 30)\n",
    "if pred == 1:\n",
    "    print(f\" AI Prediction: FAIL (Bug Detected)\")\n",
    "else:\n",
    "    print(f\" AI Prediction: PASS (Works Fine)\")\n",
    "    \n",
    "print(f\" Confidence:    {conf:.1%}\")\n",
    "\n",
    "if pred == actual_label:\n",
    "    print(\" CORRECT!\")\n",
    "else:\n",
    "    print(\" WRONG (Likely due to overfitting)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8601291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ultralytics\n",
      "  Downloading ultralytics-8.3.237-py3-none-any.whl (1.2 MB)\n",
      "     ---------------------------------------- 1.2/1.2 MB 2.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.23.0 in c:\\users\\janak\\onedrive\\desktop\\videoanalyzer\\.venv\\lib\\site-packages (from ultralytics) (2.2.6)\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in c:\\users\\janak\\onedrive\\desktop\\videoanalyzer\\.venv\\lib\\site-packages (from ultralytics) (3.10.8)\n",
      "Requirement already satisfied: opencv-python>=4.6.0 in c:\\users\\janak\\onedrive\\desktop\\videoanalyzer\\.venv\\lib\\site-packages (from ultralytics) (4.12.0.88)\n",
      "Requirement already satisfied: pillow>=7.1.2 in c:\\users\\janak\\onedrive\\desktop\\videoanalyzer\\.venv\\lib\\site-packages (from ultralytics) (11.3.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in c:\\users\\janak\\onedrive\\desktop\\videoanalyzer\\.venv\\lib\\site-packages (from ultralytics) (6.0.3)\n",
      "Requirement already satisfied: requests>=2.23.0 in c:\\users\\janak\\onedrive\\desktop\\videoanalyzer\\.venv\\lib\\site-packages (from ultralytics) (2.32.5)\n",
      "Requirement already satisfied: scipy>=1.4.1 in c:\\users\\janak\\onedrive\\desktop\\videoanalyzer\\.venv\\lib\\site-packages (from ultralytics) (1.16.3)\n",
      "Requirement already satisfied: torch>=1.8.0 in c:\\users\\janak\\onedrive\\desktop\\videoanalyzer\\.venv\\lib\\site-packages (from ultralytics) (2.6.0+cu124)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in c:\\users\\janak\\onedrive\\desktop\\videoanalyzer\\.venv\\lib\\site-packages (from ultralytics) (0.21.0+cu124)\n",
      "Requirement already satisfied: psutil>=5.8.0 in c:\\users\\janak\\onedrive\\desktop\\videoanalyzer\\.venv\\lib\\site-packages (from ultralytics) (7.1.3)\n",
      "Collecting polars>=0.20.0\n",
      "  Downloading polars-1.36.1-py3-none-any.whl (802 kB)\n",
      "     -------------------------------------- 802.4/802.4 kB 3.0 MB/s eta 0:00:00\n",
      "Collecting ultralytics-thop>=2.0.18\n",
      "  Downloading ultralytics_thop-2.0.18-py3-none-any.whl (28 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\janak\\onedrive\\desktop\\videoanalyzer\\.venv\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\janak\\onedrive\\desktop\\videoanalyzer\\.venv\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\janak\\onedrive\\desktop\\videoanalyzer\\.venv\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (4.61.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\janak\\onedrive\\desktop\\videoanalyzer\\.venv\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\janak\\onedrive\\desktop\\videoanalyzer\\.venv\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (25.0)\n",
      "Requirement already satisfied: pyparsing>=3 in c:\\users\\janak\\onedrive\\desktop\\videoanalyzer\\.venv\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\janak\\onedrive\\desktop\\videoanalyzer\\.venv\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
      "Collecting polars-runtime-32==1.36.1\n",
      "  Downloading polars_runtime_32-1.36.1-cp39-abi3-win_amd64.whl (44.5 MB)\n",
      "     ---------------------------------------- 44.5/44.5 MB 4.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\janak\\onedrive\\desktop\\videoanalyzer\\.venv\\lib\\site-packages (from requests>=2.23.0->ultralytics) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\janak\\onedrive\\desktop\\videoanalyzer\\.venv\\lib\\site-packages (from requests>=2.23.0->ultralytics) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\janak\\onedrive\\desktop\\videoanalyzer\\.venv\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2.6.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\janak\\onedrive\\desktop\\videoanalyzer\\.venv\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2025.11.12)\n",
      "Requirement already satisfied: filelock in c:\\users\\janak\\onedrive\\desktop\\videoanalyzer\\.venv\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\janak\\onedrive\\desktop\\videoanalyzer\\.venv\\lib\\site-packages (from torch>=1.8.0->ultralytics) (4.15.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\janak\\onedrive\\desktop\\videoanalyzer\\.venv\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\janak\\onedrive\\desktop\\videoanalyzer\\.venv\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\janak\\onedrive\\desktop\\videoanalyzer\\.venv\\lib\\site-packages (from torch>=1.8.0->ultralytics) (2025.9.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\janak\\onedrive\\desktop\\videoanalyzer\\.venv\\lib\\site-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\janak\\onedrive\\desktop\\videoanalyzer\\.venv\\lib\\site-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\janak\\onedrive\\desktop\\videoanalyzer\\.venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\janak\\onedrive\\desktop\\videoanalyzer\\.venv\\lib\\site-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.5)\n",
      "Installing collected packages: polars-runtime-32, polars, ultralytics-thop, ultralytics\n",
      "Successfully installed polars-1.36.1 polars-runtime-32-1.36.1 ultralytics-8.3.237 ultralytics-thop-2.0.18\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    }
   ],
   "source": [
    "%pip install ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25dd2f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading YOLO model from best.pt...\n",
      "Step 1: Analyzing user behavior (Motion Tracking)...\n",
      "User struggled most at coordinates: (315, 536)\n",
      "Step 2: Identifying the UI element (YOLO Scan)...\n",
      "\n",
      "0: 352x640 11 buttons, 6 headings, 4 images, 35.7ms\n",
      "Speed: 5.2ms preprocess, 35.7ms inference, 5.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "MATCH FOUND: User was struggling with 'image'\n",
      "Step 3: Generating Final Report Video...\n",
      "Done! Final diagnosis saved to final_diagnosis.mp4\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "VIDEO_PATH = \"dataset/screen47.mkv\"\n",
    "YOLO_MODEL_PATH = \"best.pt\" \n",
    "OUTPUT_PATH = \"final_diagnosis.mp4\"\n",
    "# ---------------------\n",
    "\n",
    "def analyze_failure_with_yolo(video_path, model_path, output_path):\n",
    "    print(f\"Loading YOLO model from {model_path}...\")\n",
    "    try:\n",
    "        model = YOLO(model_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading YOLO: {e}\")\n",
    "        print(\"Make sure you have the 'best.pt' file in your folder!\")\n",
    "        return\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    # --- STEP 1: FIND THE 'RAGE CLICK' ZONE (HEATMAP) ---\n",
    "    print(\"Step 1: Analyzing user behavior (Motion Tracking)...\")\n",
    "    \n",
    "    ret, first_frame = cap.read()\n",
    "    if not ret: return\n",
    "\n",
    "    heatmap = np.zeros((first_frame.shape[0], first_frame.shape[1]), dtype=np.float32)\n",
    "    prev_gray = cv2.cvtColor(first_frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret: break\n",
    "        \n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        diff = cv2.absdiff(prev_gray, gray)\n",
    "        _, thresh = cv2.threshold(diff, 15, 255, cv2.THRESH_BINARY)\n",
    "        heatmap = cv2.add(heatmap, thresh.astype(np.float32))\n",
    "        prev_gray = gray\n",
    "\n",
    "    # Find the precise (X, Y) coordinate of the struggle\n",
    "    heatmap_blur = cv2.GaussianBlur(heatmap, (51, 51), 0)\n",
    "    (minVal, maxVal, minLoc, maxLoc) = cv2.minMaxLoc(heatmap_blur)\n",
    "    struggle_point = maxLoc # (x, y)\n",
    "    \n",
    "    print(f\"User struggled most at coordinates: {struggle_point}\")\n",
    "\n",
    "    # --- STEP 2: IDENTIFY THE FEATURE (YOLO) ---\n",
    "    print(\"Step 2: Identifying the UI element (YOLO Scan)...\")\n",
    "    \n",
    "    # We grab the frame where the heatmap was hottest (or just the first frame/middle frame)\n",
    "    # Usually UI elements don't move, so checking the middle frame is safe.\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, int(cap.get(cv2.CAP_PROP_FRAME_COUNT) / 2))\n",
    "    ret, frame_to_scan = cap.read()\n",
    "    \n",
    "    # Run YOLO Inference\n",
    "    results = model(frame_to_scan)\n",
    "    \n",
    "    culprit_box = None\n",
    "    culprit_name = \"Unknown Area\"\n",
    "    \n",
    "    # Check which box contains the 'Struggle Point'\n",
    "    for result in results:\n",
    "        boxes = result.boxes\n",
    "        for box in boxes:\n",
    "            # Get coordinates (x1, y1, x2, y2)\n",
    "            x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n",
    "            cls_id = int(box.cls[0])\n",
    "            label = model.names[cls_id]\n",
    "            \n",
    "            # Check intersection: Is the struggle point inside this box?\n",
    "            if x1 <= struggle_point[0] <= x2 and y1 <= struggle_point[1] <= y2:\n",
    "                culprit_box = (int(x1), int(y1), int(x2), int(y2))\n",
    "                culprit_name = label\n",
    "                print(f\"MATCH FOUND: User was struggling with '{label}'\")\n",
    "                break\n",
    "        if culprit_box: break\n",
    "\n",
    "    # --- STEP 3: VISUALIZE RESULT ---\n",
    "    print(\"Step 3: Generating Final Report Video...\")\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    # Setup Writer\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret: break\n",
    "        \n",
    "        # 1. Draw the \"Struggle Point\" (Red Crosshair)\n",
    "        cv2.drawMarker(frame, struggle_point, (0, 0, 255), cv2.MARKER_CROSS, 30, 2)\n",
    "        \n",
    "        # 2. Draw the \"Culprit Box\" (YOLO Detection)\n",
    "        if culprit_box:\n",
    "            cv2.rectangle(frame, (culprit_box[0], culprit_box[1]), (culprit_box[2], culprit_box[3]), (0, 255, 0), 3)\n",
    "            \n",
    "            # Label\n",
    "            text = f\"FAIL: {culprit_name}\"\n",
    "            cv2.putText(frame, text, (culprit_box[0], culprit_box[1]-10), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "        else:\n",
    "            # If YOLO didn't find a button there, just label the area\n",
    "            cv2.putText(frame, \"FAIL: Unrecognized Area\", (struggle_point[0], struggle_point[1]-10), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 0, 255), 2)\n",
    "\n",
    "        out.write(frame)\n",
    "        \n",
    "    cap.release()\n",
    "    out.release()\n",
    "    print(f\"Done! Final diagnosis saved to {output_path}\")\n",
    "\n",
    "# Run it\n",
    "analyze_failure_with_yolo(VIDEO_PATH, YOLO_MODEL_PATH, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ee9aa98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing video: dataset/screen47.mkv\n",
      "Step 1: Tracking motion...\n",
      "FAIL Detected at coordinates: (318, 511)\n",
      "Step 2: Generating output video...\n",
      "Done! Saved to highlighted_failure.mp4\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "VIDEO_PATH = \"dataset/screen47.mkv\"  # <--- Change this to your video file\n",
    "OUTPUT_PATH = \"highlighted_failure.mp4\"\n",
    "# ---------------------\n",
    "\n",
    "def create_failure_heatmap(video_path, output_path):\n",
    "    print(f\"Processing video: {video_path}\")\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    # 1. Read first frame to get dimensions\n",
    "    ret, first_frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error: Could not read video. Check the path.\")\n",
    "        return\n",
    "\n",
    "    # Create an empty black canvas for the heatmap\n",
    "    # float32 is used so we can accumulate values without hitting a limit of 255 immediately\n",
    "    heatmap = np.zeros((first_frame.shape[0], first_frame.shape[1]), dtype=np.float32)\n",
    "    \n",
    "    # Pre-processing\n",
    "    prev_gray = cv2.cvtColor(first_frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Get Video Properties for saving\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    \n",
    "    # Setup Video Writer\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "    \n",
    "    # --- PHASE 1: GENERATE HEATMAP ---\n",
    "    print(\"Step 1: Tracking motion...\")\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret: break\n",
    "        \n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # Calculate Difference (Motion)\n",
    "        # absdiff finds the absolute difference between current and previous frame\n",
    "        diff = cv2.absdiff(prev_gray, gray)\n",
    "        \n",
    "        # Threshold: Ignore tiny changes (lighting noise), keep only distinct motion (cursor/clicks)\n",
    "        _, thresh = cv2.threshold(diff, 15, 255, cv2.THRESH_BINARY)\n",
    "        \n",
    "        # Accumulate: Add the motion to the heatmap\n",
    "        heatmap = cv2.add(heatmap, thresh.astype(np.float32))\n",
    "        \n",
    "        prev_gray = gray\n",
    "\n",
    "    cap.release()\n",
    "    \n",
    "    # Normalization: Scale the heatmap so the hottest spot is 255 (White) and coldest is 0 (Black)\n",
    "    heatmap_norm = cv2.normalize(heatmap, None, 0, 255, cv2.NORM_MINMAX)\n",
    "    heatmap_norm = np.uint8(heatmap_norm)\n",
    "    \n",
    "    # --- PHASE 2: LOCALIZE THE BUG ---\n",
    "    # We blur the heatmap to find the general \"center of mass\" of the struggle\n",
    "    heatmap_blur = cv2.GaussianBlur(heatmap_norm, (51, 51), 0)\n",
    "    (minVal, maxVal, minLoc, maxLoc) = cv2.minMaxLoc(heatmap_blur)\n",
    "    \n",
    "    # maxLoc is the coordinate (x, y) where the user moved/clicked the most\n",
    "    print(f\"FAIL Detected at coordinates: {maxLoc}\")\n",
    "    \n",
    "    # --- PHASE 3: DRAW OVERLAY ---\n",
    "    print(\"Step 2: Generating output video...\")\n",
    "    \n",
    "    # Convert grayscale heatmap to a colorful heat map (Blue=Cold, Red=Hot)\n",
    "    heatmap_color = cv2.applyColorMap(heatmap_norm, cv2.COLORMAP_JET)\n",
    "    \n",
    "    # Re-open video to draw on top of it\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret: break\n",
    "        \n",
    "        # Blend: 70% Original Video + 30% Heatmap\n",
    "        overlay = cv2.addWeighted(frame, 0.7, heatmap_color, 0.3, 0)\n",
    "        \n",
    "        # Draw a Box around the \"Hottest\" spot (The detected bug)\n",
    "        # We assume the button is roughly 100x100 pixels\n",
    "        box_size = 50\n",
    "        top_left = (maxLoc[0] - box_size, maxLoc[1] - box_size)\n",
    "        bottom_right = (maxLoc[0] + box_size, maxLoc[1] + box_size)\n",
    "        \n",
    "        # Draw Rectangle\n",
    "        cv2.rectangle(overlay, top_left, bottom_right, (0, 0, 255), 3)\n",
    "        \n",
    "        # Draw Label\n",
    "        cv2.putText(overlay, \"FAIL AREA\", (top_left[0], top_left[1]-10), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 0, 255), 2)\n",
    "        \n",
    "        # Write frame to file\n",
    "        out.write(overlay)\n",
    "        \n",
    "    cap.release()\n",
    "    out.release()\n",
    "    print(f\"Done! Saved to {output_path}\")\n",
    "\n",
    "# Run the function\n",
    "create_failure_heatmap(VIDEO_PATH, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "231e0af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\janak\\OneDrive\\Desktop\\VideoAnalyzer\\.venv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Analyzing User Experience...\n",
      "Face: dataset/cam47.mp4\n",
      "Screen: dataset/screen47.mp4\n",
      "--------------------------------------------------\n",
      "TIME       | EMOTION      | STATUS    \n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Analysis Complete.\n",
      "Timeline saved to: ux_report/ux_timeline.csv\n",
      "Failure Screenshots saved in: ux_report/\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from deepface import DeepFace\n",
    "import pandas as pd\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "CAM_VIDEO = \"dataset/cam47.mp4\"      # The User's Face\n",
    "SCREEN_VIDEO = \"dataset/screen47.mp4\" # The Screen Recording\n",
    "OUTPUT_FOLDER = \"ux_report\"           # Where to save evidence\n",
    "SAMPLE_RATE = 1                       # Check emotion every 1 second\n",
    "# ---------------------\n",
    "\n",
    "def format_time(seconds):\n",
    "    return str(datetime.timedelta(seconds=int(seconds)))\n",
    "\n",
    "def generate_ux_timeline(cam_path, screen_path, output_folder):\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "        \n",
    "    print(f\"Analyzing User Experience...\")\n",
    "    print(f\"Face: {cam_path}\")\n",
    "    print(f\"Screen: {screen_path}\")\n",
    "    \n",
    "    cap_cam = cv2.VideoCapture(cam_path)\n",
    "    cap_screen = cv2.VideoCapture(screen_path)\n",
    "    \n",
    "    fps = cap_cam.get(cv2.CAP_PROP_FPS)\n",
    "    if fps == 0: fps = 30 # Fallback\n",
    "    \n",
    "    # Define Emotion Categories\n",
    "    # Success = Good UI\n",
    "    # Fail = Bad UI (Frustration)\n",
    "    positive_emotions = ['happy', 'surprise'] \n",
    "    negative_emotions = ['angry', 'disgust', 'fear', 'sad']\n",
    "    \n",
    "    timeline = []\n",
    "    frame_count = 0\n",
    "    \n",
    "    print(\"-\" * 50)\n",
    "    print(f\"{'TIME':<10} | {'EMOTION':<12} | {'STATUS':<10}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    while True:\n",
    "        # Jump to next second (skip frames to save time)\n",
    "        # We process 1 frame every 'SAMPLE_RATE' seconds\n",
    "        frames_to_skip = int(fps * SAMPLE_RATE)\n",
    "        cap_cam.set(cv2.CAP_PROP_POS_FRAMES, frame_count)\n",
    "        \n",
    "        ret_cam, frame_cam = cap_cam.read()\n",
    "        if not ret_cam: break\n",
    "        \n",
    "        current_time_sec = frame_count / fps\n",
    "        timestamp = format_time(current_time_sec)\n",
    "        \n",
    "        try:\n",
    "            # 1. Detect Emotion\n",
    "            # We turn off 'enforce_detection' so it doesn't crash if user looks away\n",
    "            result = DeepFace.analyze(frame_cam, actions=['emotion'], \n",
    "                                    enforce_detection=False, silent=True)\n",
    "            \n",
    "            if result:\n",
    "                emotion = result[0]['dominant_emotion']\n",
    "                confidence = result[0]['face_confidence']\n",
    "                \n",
    "                # Filter out low confidence (face partially hidden)\n",
    "                if confidence > 0.5:\n",
    "                    status = \"NEUTRAL\"\n",
    "                    is_incident = False\n",
    "                    \n",
    "                    if emotion in positive_emotions:\n",
    "                        status = \"SUCCESS\"\n",
    "                    elif emotion in negative_emotions:\n",
    "                        status = \"FAILURE\"\n",
    "                        is_incident = True\n",
    "                    \n",
    "                    # Log to Console\n",
    "                    print(f\"{timestamp:<10} | {emotion:<12} | {status}\")\n",
    "                    \n",
    "                    # 2. If FAILURE detected, grab the Screen Evidence\n",
    "                    if is_incident:\n",
    "                        # Jump screen video to exact same time\n",
    "                        cap_screen.set(cv2.CAP_PROP_POS_MSEC, current_time_sec * 1000)\n",
    "                        ret_scr, frame_scr = cap_screen.read()\n",
    "                        \n",
    "                        if ret_scr:\n",
    "                            # Save the screen image\n",
    "                            filename = f\"{output_folder}/fail_{int(current_time_sec)}s_{emotion}.jpg\"\n",
    "                            cv2.imwrite(filename, frame_scr)\n",
    "                            \n",
    "                    timeline.append({\n",
    "                        \"time_sec\": current_time_sec,\n",
    "                        \"timestamp\": timestamp,\n",
    "                        \"emotion\": emotion,\n",
    "                        \"status\": status\n",
    "                    })\n",
    "                    \n",
    "        except Exception as e:\n",
    "            pass # Skip frames where DeepFace gets confused\n",
    "            \n",
    "        # Move to next second\n",
    "        frame_count += frames_to_skip\n",
    "\n",
    "    cap_cam.release()\n",
    "    cap_screen.release()\n",
    "    \n",
    "    # Save Report to CSV\n",
    "    df = pd.DataFrame(timeline)\n",
    "    csv_path = f\"{output_folder}/ux_timeline.csv\"\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    \n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Analysis Complete.\")\n",
    "    print(f\"Timeline saved to: {csv_path}\")\n",
    "    print(f\"Failure Screenshots saved in: {output_folder}/\")\n",
    "\n",
    "# Run it\n",
    "generate_ux_timeline(CAM_VIDEO, SCREEN_VIDEO, OUTPUT_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6166d652",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
